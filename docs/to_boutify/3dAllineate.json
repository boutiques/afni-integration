{"helptext": ["Usage: 3dAllineate [options] sourcedataset", "", "Program to align one dataset (the 'source') to a base dataset.", "Options are available to control:", " ++ How the matching between the source and the base is computed", "    (i.e., the 'cost functional' measuring image mismatch).", " ++ How the resliced source is interpolated to the base space.", " ++ The complexity of the spatial transformation ('warp') used.", " ++ And many many technical options to control the process in detail,", "    if you know what you are doing (or just like to fool around).", "", "=====----------------------------------------------------------------------", "NOTES: For most 3D image registration purposes, we now recommend that you", "=====  use Daniel Glen's script align_epi_anat.py (which, despite its name,", "       can do many more registration problems than EPI-to-T1-weighted).", "  -->> In particular, using 3dAllineate with the 'lpc' cost functional", "       (to align EPI and T1-weighted volumes) requires using a '-weight'", "       volume to get good results, and the align_epi_anat.py script will", "       automagically generate such a weight dataset that works well for", "       EPI-to-structural alignment.", "  -->> This script can also be used for other alignment purposes, such", "       as T1-weighted alignment between field strengths using the", "       '-lpa' cost functional.  Investigate align_epi_anat.py to", "       see if it will do what you need -- you might make your life", "       a little easier and nicer and happier and more tranquil.", "  -->> Also, if/when you ask for registration help on the AFNI", "       message board, we'll probably start by recommending that you", "       try align_epi_anat.py if you haven't already done so.", "  -->> For aligning EPI and T1-weighted volumes, we have found that", "       using a flip angle of 50-60 degrees for the EPI works better than", "       a flip angle of 90 degrees.  The reason is that there is more", "       internal contrast in the EPI data when the flip angle is smaller,", "       so the registration has some image structure to work with.  With", "       the 90 degree flip angle, there is so little internal contrast in", "       the EPI dataset that the alignment process ends up being just", "       trying to match brain outlines -- which doesn't always give accurate", "       results: see http://dx.doi.org/10.1016/j.neuroimage.2008.09.037", "  -->> Although the total MRI signal is reduced at a smaller flip angle,", "       there is little or no loss in FMRI/BOLD information, since the bulk", "       of the time series 'noise' is from physiological fluctuation signals,", "       which are also reduced by the lower flip angle -- for more details,", "       see http://dx.doi.org/10.1016/j.neuroimage.2010.11.020", "---------------------------------------------------------------------------", "  **** New (Summer 2013) program 3dQwarp is available to do nonlinear  ****", "  ***  alignment between a base and source dataset, including the use   ***", "  **   of 3dAllineate for the preliminary affine alignment.  If you are  **", "  *    interested, see the output of '3dQwarp -help' for the details.     *", "---------------------------------------------------------------------------", "", "COMMAND LINE OPTIONS:", "====================", " -base bbb   = Set the base dataset to be the #0 sub-brick of 'bbb'.", "               If no -base option is given, then the base volume is", "               taken to be the #0 sub-brick of the source dataset.", "               (Base must be stored as floats, shorts, or bytes.)", "", " -source ttt = Read the source dataset from 'ttt'.  If no -source", "   *OR*        (or -input) option is given, then the source dataset", " -input ttt    is the last argument on the command line.", "               (Source must be stored as floats, shorts, or bytes.)", "            ** 3dAllineate can register 2D datasets (single slice),", "               but both the base and source must be 2D -- you cannot", "               use this program to register a 2D slice into a 3D volume!", "            ** See the script @2dwarper.Allin for an example of using", "               3dAllineate to do slice-by-slice nonlinear warping to", "               align 3D volumes distorted by time-dependent magnetic", "               field inhomogeneities.", "", " ** NOTA BENE: The base and source dataset do NOT have to be defined **", " ** [that's]   on the same 3D grids; the alignment process uses the  **", " ** [Latin ]   coordinate systems defined in the dataset headers to  **", " ** [  for ]   make the match between spatial locations, rather than **", " ** [ NOTE ]   matching the 2 datasets on a voxel-by-voxel basis     **", " ** [ WELL ]   (as 3dvolreg and 3dWarpDrive do).                     **", " **       -->> However, this coordinate-based matching requires that **", " **            image volumes be defined on roughly the same patch of **", " **            of (x,y,z) space, in order to find a decent starting  **", " **            point for the transformation.  You might need to use  **", " **            the script @Align_Centers to do this, if the 3D       **", " **            spaces occupied by the images do not overlap much.    **", " **       -->> Or the '-cmass' option to this program might be       **", " **            sufficient to solve this problem, maybe, with luck.   **", " **            (Another reason why you should use align_epi_anat.py) **", " **       -->> If the coordinate system in the dataset headers is    **", " **            WRONG, then 3dAllineate will probably not work well!  **", "", " -prefix ppp = Output the resulting dataset to file 'ppp'.  If this", "   *OR*        option is NOT given, no dataset will be output!  The", " -out ppp      transformation matrix to align the source to the base will", "               be estimated, but not applied.  You can save the matrix", "               for later use using the '-1Dmatrix_save' option.", "        *N.B.: By default, the new dataset is computed on the grid of the", "                base dataset; see the '-master' and/or the '-mast_dxyz'", "                options to change this grid.", "        *N.B.: If 'ppp' is 'NULL', then no output dataset will be produced.", "                This option is for compatibility with 3dvolreg.", "", " -floatize   = Write result dataset as floats.  Internal calculations", " -float        are all done on float copies of the input datasets.", "               [Default=convert output dataset to data format of  ]", "               [        source dataset; if the source dataset was ]", "               [        shorts with a scale factor, then the new  ]", "               [        dataset will get a scale factor as well;  ]", "               [        if the source dataset was shorts with no  ]", "               [        scale factor, the result will be unscaled.]", "", " -1Dparam_save ff   = Save the warp parameters in ASCII (.1D) format into", "                      file 'ff' (1 row per sub-brick in source).", "                    * A historical synonym for this option is '-1Dfile'.", "                    * At the top of the saved 1D file is a #comment line", "                      listing the names of the parameters; those parameters", "                      that are fixed (e.g., via '-parfix') will be marked", "                      by having their symbolic names end in the '$' character.", "                      You can use '1dcat -nonfixed' to remove these columns", "                      from the 1D file if you just want to further process the", "                      varying parameters somehow (e.g., 1dsvd).", "                    * However, the '-1Dparam_apply' option requires the", "                      full list of parameters, including those that were", "                      fixed, in order to work properly!", "", " -1Dparam_apply aa  = Read warp parameters from file 'aa', apply them to ", "                      the source dataset, and produce a new dataset.", "                      (Must also use the '-prefix' option for this to work!  )", "                      (In this mode of operation, there is no optimization of)", "                      (the cost functional by changing the warp parameters;  )", "                      (previously computed parameters are applied directly.  )", "               *N.B.: A historical synonym for this is '-1Dapply'.", "               *N.B.: If you use -1Dparam_apply, you may also want to use", "                       -master to control the grid on which the new", "                       dataset is written -- the base dataset from the", "                       original 3dAllineate run would be a good possibility.", "                       Otherwise, the new dataset will be written out on the", "                       3D grid coverage of the source dataset, and this", "                       might result in clipping off part of the image.", "               *N.B.: Each row in the 'aa' file contains the parameters for", "                       transforming one sub-brick in the source dataset.", "                       If there are more sub-bricks in the source dataset", "                       than there are rows in the 'aa' file, then the last", "                       row is used repeatedly.", "               *N.B.: A trick to use 3dAllineate to resample a dataset to", "                       a finer grid spacing:", "                         3dAllineate -input dataset+orig         \\", "                                     -master template+orig       \\", "                                     -prefix newdataset          \\", "                                     -final wsinc5               \\", "                                     -1Dparam_apply '1D: 12@0'\\'  ", "                       Here, the identity transformation is specified", "                       by giving all 12 affine parameters as 0 (note", "                       the extra \\' at the end of the '1D: 12@0' input!).", "                     ** You can also use the word 'IDENTITY' in place of", "                        '1D: 12@0'\\' (to indicate the identity transformation).", "              **N.B.: Some expert options for modifying how the wsinc5", "                       method works are described far below, if you use", "                       '-HELP' instead of '-help'.", "            ****N.B.: The interpolation method used to produce a dataset", "                       is always given via the '-final' option, NOT via", "                       '-interp'.  If you forget this and use '-interp'", "                       along with one of the 'apply' options, this program", "                       will chastise you (gently) and change '-final'", "                       to match what the '-interp' input.", "", " -1Dmatrix_save ff  = Save the transformation matrix for each sub-brick into", "                      file 'ff' (1 row per sub-brick in the source dataset).", "                      If 'ff' does NOT end in '.1D', then the program will", "                      append '.aff12.1D' to 'ff' to make the output filename.", "               *N.B.: This matrix is the coordinate transformation from base", "                       to source DICOM coordinates. In other terms:", "                          Xin = Xsource = M Xout = M Xbase", "                                   or", "                          Xout = Xbase = inv(M) Xin = inv(M) Xsource", "                       where Xin or Xsource is the 4x1 coordinates of a", "                       location in the input volume. Xout is the ", "                       coordinate of that same location in the output volume.", "                       Xbase is the coordinate of the corresponding location", "                       in the base dataset. M is ff augmented by a 4th row of", "                       [0 0 0 1], X. is an augmented column vector [x,y,z,1]'", "                       To get the inverse matrix inv(M)", "                       (source to base), use the cat_matvec program, as in", "                         cat_matvec fred.aff12.1D -I", "", " -1Dmatrix_apply aa = Use the matrices in file 'aa' to define the spatial", "                      transformations to be applied.  Also see program", "                      cat_matvec for ways to manipulate these matrix files.", "               *N.B.: You probably want to use either -base or -master", "                      with either *_apply option, so that the coordinate", "                      system that the matrix refers to is correctly loaded.", "                     ** You can also use the word 'IDENTITY' in place of a", "                        filename to indicate the identity transformation --", "                        presumably for the purpose of resampling the source", "                        dataset to a new grid.", "", "  * The -1Dmatrix_* options can be used to save and re-use the transformation *", "  * matrices.  In combination with the program cat_matvec, which can multiply *", "  * saved transformation matrices, you can also adjust these matrices to      *", "  * other alignments.                                                         *", "", "  * The script 'align_epi_anat.py' uses 3dAllineate and 3dvolreg to align EPI *", "  * datasets to T1-weighted anatomical datasets, using saved matrices between *", "  * the two programs.  This script is our currently recommended method for    *", "  * doing such intra-subject alignments.                                      *", "", " -cost ccc   = Defines the 'cost' function that defines the matching", "               between the source and the base; 'ccc' is one of", "                ls   *OR*  leastsq         = Least Squares [Pearson Correlation]", "                mi   *OR*  mutualinfo      = Mutual Information [H(b)+H(s)-H(b,s)]", "                crM  *OR*  corratio_mul    = Correlation Ratio (Symmetrized*)", "                nmi  *OR*  norm_mutualinfo = Normalized MI [H(b,s)/(H(b)+H(s))]", "                hel  *OR*  hellinger       = Hellinger metric", "                crA  *OR*  corratio_add    = Correlation Ratio (Symmetrized+)", "                crU  *OR*  corratio_uns    = Correlation Ratio (Unsym)", "               You can also specify the cost functional using an option", "               of the form '-mi' rather than '-cost mi', if you like", "               to keep things terse and cryptic (as I do).", "               [Default == '-hel' (for no good reason, but it sounds nice).]", "", " -interp iii = Defines interpolation method to use during matching", "               process, where 'iii' is one of", "                 NN      *OR* nearestneighbour *OR nearestneighbor", "                 linear  *OR* trilinear", "                 cubic   *OR* tricubic", "                 quintic *OR* triquintic", "               Using '-NN' instead of '-interp NN' is allowed (e.g.).", "               Note that using cubic or quintic interpolation during", "               the matching process will slow the program down a lot.", "               Use '-final' to affect the interpolation method used", "               to produce the output dataset, once the final registration", "               parameters are determined.  [Default method == 'linear'.]", "            ** N.B.: Linear interpolation is used during the coarse", "                     alignment pass; the selection here only affects", "                     the interpolation method used during the second", "                     (fine) alignment pass.", "            ** N.B.: '-interp' does NOT define the final method used", "                     to produce the output dataset as warped from the", "                     input dataset.  If you want to do that, use '-final'.", "", " -final iii  = Defines the interpolation mode used to create the", "               output dataset.  [Default == 'cubic']", "            ** N.B.: For '-final' ONLY, you can use 'wsinc5' to specify", "                       that the final interpolation be done using a", "                       weighted sinc interpolation method.  This method", "                       is so SLOW that you aren't allowed to use it for", "                       the registration itself.", "                  ++ wsinc5 interpolation is highly accurate and should", "                       reduce the smoothing artifacts from lower", "                       order interpolation methods (which are most", "                       visible if you interpolate an EPI time series", "                       to high resolution and then make an image of", "                       the voxel-wise variance).", "                  ++ On my Intel-based Mac, it takes about 2.5 s to do", "                       wsinc5 interpolation, per 1 million voxels output.", "                       For comparison, quintic interpolation takes about", "                       0.3 s per 1 million voxels: 8 times faster than wsinc5.", "                  ++ The '5' refers to the width of the sinc interpolation", "                       weights: plus/minus 5 grid points in each direction;", "                       this is a tensor product interpolation, for speed.", "", "TECHNICAL OPTIONS (used for fine control of the program):", "=================", " -nmatch nnn = Use at most 'nnn' scattered points to match the", "               datasets.  The smaller nnn is, the faster the matching", "               algorithm will run; however, accuracy may be bad if", "               nnn is too small.  If you end the 'nnn' value with the", "               '%' character, then that percentage of the base's", "               voxels will be used.", "               [Default == 47% of voxels in the weight mask]", "", " -nopad      = Do not use zero-padding on the base image.", "               [Default == zero-pad, if needed; -verb shows how much]", "", " -zclip      = Replace negative values in the input datasets (source & base)", "               with zero.  The intent is to clip off a small set of negative", "               values that may arise when using 3dresample (say) with", "               cubic interpolation.", "", " -conv mmm   = Convergence test is set to 'mmm' millimeters.", "               This doesn't mean that the results will be accurate", "               to 'mmm' millimeters!  It just means that the program", "               stops trying to improve the alignment when the optimizer", "               (NEWUOA) reports it has narrowed the search radius", "               down to this level.  [Default == 0.05 mm]", "", " -verb       = Print out verbose progress reports.", "               [Using '-VERB' will give even more prolix reports.]", " -quiet      = Don't print out verbose stuff.", " -usetemp    = Write intermediate stuff to disk, to economize on RAM.", "               Using this will slow the program down, but may make it", "               possible to register datasets that need lots of space.", "       **N.B.: Temporary files are written to the directory given", "               in environment variable TMPDIR, or in /tmp, or in ./", "               (preference in that order).  If the program crashes,", "               these files are named TIM_somethingrandom, and you", "               may have to delete them manually. (TIM=Temporary IMage)", "       **N.B.: If the program fails with a 'malloc failure' type of", "               message, then try '-usetemp' (malloc=memory allocator).", " -nousetemp  = Don't use temporary workspace on disk [the default].", "", " -check hhh  = After cost functional optimization is done, start at the", "               final parameters and RE-optimize using the new cost", "               function 'hhh'.  If the results are too different, a", "               warning message will be printed.  However, the final", "               parameters from the original optimization will be", "               used to create the output dataset. Using '-check'", "               increases the CPU time, but can help you feel sure", "               that the alignment process did not go wild and crazy.", "               [Default == no check == don't worry, be happy!]", "       **N.B.: You can put more than one function after '-check', as in", "                 -nmi -check mi hel crU crM", "               to register with Normalized Mutual Information, and", "               then check the results against 4 other cost functionals.", "       **N.B.: On the other hand, some cost functionals give better", "               results than others for specific problems, and so", "               a warning that 'mi' was significantly different than", "               'hel' might not actually mean anything useful (e.g.).", "", " ** PARAMETERS THAT AFFECT THE COST OPTIMIZATION STRATEGY **", " -onepass    = Use only the refining pass -- do not try a coarse", "               resolution pass first.  Useful if you know that only", "               small amounts of image alignment are needed.", "               [The default is to use both passes.]", " -twopass    = Use a two pass alignment strategy, first searching for", "               a large rotation+shift and then refining the alignment.", "               [Two passes are used by default for the first sub-brick]", "               [in the source dataset, and then one pass for the others.]", "               ['-twopass' will do two passes for ALL source sub-bricks.]", " -twoblur rr = Set the blurring radius for the first pass to 'rr'", "               millimeters.  [Default == 11 mm]", "       **N.B.: You may want to change this from the default if", "               your voxels are unusually small or unusually large", "               (e.g., outside the range 1-4 mm along each axis).", " -twofirst   = Use -twopass on the first image to be registered, and", "               then on all subsequent images from the source dataset,", "               use results from the first image's coarse pass to start", "               the fine pass.", "               (Useful when there may be large motions between the   )", "               (source and the base, but only small motions within   )", "               (the source dataset itself; since the coarse pass can )", "               (be slow, doing it only once makes sense in this case.)", "       **N.B.: [-twofirst is on by default; '-twopass' turns it off.]", " -twobest bb = In the coarse pass, use the best 'bb' set of initial", "               points to search for the starting point for the fine", "               pass.  If bb==0, then no search is made for the best", "               starting point, and the identity transformation is", "               used as the starting point.  [Default=5; min=0 max=22]", "       **N.B.: Setting bb=0 will make things run faster, but less reliably.", " -fineblur x = Set the blurring radius to use in the fine resolution", "               pass to 'x' mm.  A small amount (1-2 mm?) of blurring at", "               the fine step may help with convergence, if there is", "               some problem, especially if the base volume is very noisy.", "               [Default == 0 mm = no blurring at the final alignment pass]", "   **NOTES ON", "   **STRATEGY: * If you expect only small-ish (< 2 voxels?) image movement,", "                 then using '-onepass' or '-twobest 0' makes sense.", "               * If you expect large-ish image movements, then do not", "                 use '-onepass' or '-twobest 0'; the purpose of the", "                 '-twobest' parameter is to search for large initial", "                 rotations/shifts with which to start the coarse", "                 optimization round.", "               * If you have multiple sub-bricks in the source dataset,", "                 then the default '-twofirst' makes sense if you don't expect", "                 large movements WITHIN the source, but expect large motions", "                 between the source and base.", "               * '-twopass' re-starts the alignment process for each sub-brick", "                 in the source dataset -- this option can be time consuming,", "                 and is really intended to be used when you might expect large", "                 movements between sub-bricks; for example, when the different", "                 volumes are gathered on different days.  For most purposes,", "                 '-twofirst' (the default process) will be adequate and faster,", "                 when operating on multi-volume source datasets.", "", " -cmass        = Use the center-of-mass calculation to bracket the shifts.", "                   [This option is OFF by default]", "                 If given in the form '-cmass+xy' (for example), means to", "                 do the CoM calculation in the x- and y-directions, but", "                 not the z-direction.", " -nocmass      = Don't use the center-of-mass calculation. [The default]", "                  (You would not want to use the C-o-M calculation if the  )", "                  (source sub-bricks have very different spatial locations,)", "                  (since the source C-o-M is calculated from all sub-bricks)", " **EXAMPLE: You have a limited coverage set of axial EPI slices you want to", "            register into a larger head volume (after 3dSkullStrip, of course).", "            In this case, '-cmass+xy' makes sense, allowing CoM adjustment", "            along the x = R-L and y = A-P directions, but not along the", "            z = I-S direction, since the EPI doesn't cover the whole brain", "            along that axis.", "", " -autoweight = Compute a weight function using the 3dAutomask", "               algorithm plus some blurring of the base image.", "       **N.B.: '-autoweight+100' means to zero out all voxels", "                 with values below 100 before computing the weight.", "               '-autoweight**1.5' means to compute the autoweight", "                 and then raise it to the 1.5-th power (e.g., to", "                 increase the weight of high-intensity regions).", "               These two processing steps can be combined, as in", "                 '-autoweight+100**1.5'", "               ** Note that that '**' must be enclosed in quotes;", "                  otherwise, the shell will treat it as a wildcard", "                  and you will get an error message before 3dAllineate", "                  even starts!!", "       **N.B.: Some cost functionals do not allow -autoweight, and", "               will use -automask instead.  A warning message", "               will be printed if you run into this situation.", "               If a clip level '+xxx' is appended to '-autoweight',", "               then the conversion into '-automask' will NOT happen.", "               Thus, using a small positive '+xxx' can be used trick", "               -autoweight into working on any cost functional.", " -automask   = Compute a mask function, which is like -autoweight,", "               but the weight for a voxel is set to either 0 or 1.", "       **N.B.: '-automask+3' means to compute the mask function, and", "               then dilate it outwards by 3 voxels (e.g.).", "               ** Note that '+' means something very different", "                  for '-automask' and '-autoweight'!!", " -autobox    = Expand the -automask function to enclose a rectangular", "               box that holds the irregular mask.", "       **N.B.: This is the default mode of operation!", "               For intra-modality registration, '-autoweight' may be better!", "             * If the cost functional is 'ls', then '-autoweight' will be", "               the default, instead of '-autobox'.", " -nomask     = Don't compute the autoweight/mask; if -weight is not", "               also used, then every voxel will be counted equally.", " -weight www = Set the weighting for each voxel in the base dataset;", "               larger weights mean that voxel counts more in the cost", "               function.", "       **N.B.: The weight dataset must be defined on the same grid as", "               the base dataset.", "       **N.B.: Even if a method does not allow -autoweight, you CAN", "               use a weight dataset that is not 0/1 valued.  The", "               risk is yours, of course (!*! as always in AFNI !*!).", " -wtprefix p = Write the weight volume to disk as a dataset with", "               prefix name 'p'.  Used with '-autoweight/mask', this option", "               lets you see what voxels were important in the algorithm.", " -emask ee   = This option lets you specify a mask of voxels to EXCLUDE from", "               the analysis. The voxels where the dataset 'ee' is nonzero", "               will not be included (i.e., their weights will be set to zero).", "             * Like all the weight options, it applies in the base image", "               coordinate system.", "             * Like all the weight options, it means nothing if you are using", "               one of the 'apply' options.", "", "    Method  Allows -autoweight", "    ------  ------------------", "     ls     YES", "     mi     NO", "     crM    YES", "     nmi    NO", "     hel    NO", "     crA    YES", "     crU    YES", "", " -source_mask sss = Mask the source (input) dataset, using 'sss'.", " -source_automask = Automatically mask the source dataset.", "                      [By default, all voxels in the source]", "                      [dataset are used in the matching.   ]", "            **N.B.: You can also use '-source_automask+3' to dilate", "                    the default source automask outward by 3 voxels.", "", " -warp xxx   = Set the warp type to 'xxx', which is one of", "                 shift_only         *OR* sho =  3 parameters", "                 shift_rotate       *OR* shr =  6 parameters", "                 shift_rotate_scale *OR* srs =  9 parameters", "                 affine_general     *OR* aff = 12 parameters", "               [Default = affine_general, which includes image]", "               [      shifts, rotations, scaling, and shearing]", "", " -warpfreeze = Freeze the non-rigid body parameters (those past #6)", "               after doing the first sub-brick.  Subsequent volumes", "               will have the same spatial distortions as sub-brick #0,", "               plus rigid body motions only.", "", " -replacebase   = If the source has more than one sub-brick, and this", "                  option is turned on, then after the #0 sub-brick is", "                  aligned to the base, the aligned #0 sub-brick is used", "                  as the base image for subsequent source sub-bricks.", "", " -replacemeth m = After sub-brick #0 is aligned, switch to method 'm'", "                  for later sub-bricks.  For use with '-replacebase'.", "", " -EPI        = Treat the source dataset as being composed of warped", "               EPI slices, and the base as comprising anatomically", "               'true' images.  Only phase-encoding direction image", "               shearing and scaling will be allowed with this option.", "       **N.B.: For most people, the base dataset will be a 3dSkullStrip-ed", "               T1-weighted anatomy (MPRAGE or SPGR).  If you don't remove", "               the skull first, the EPI images (which have little skull", "               visible due to fat-suppression) might expand to fit EPI", "               brain over T1-weighted skull.", "       **N.B.: Usually, EPI datasets don't have as complete slice coverage", "               of the brain as do T1-weighted datasets.  If you don't use", "               some option (like '-EPI') to suppress scaling in the slice-", "               direction, the EPI dataset is likely to stretch the slice", "               thicknesss to better 'match' the T1-weighted brain coverage.", "       **N.B.: '-EPI' turns on '-warpfreeze -replacebase'.", "               You can use '-nowarpfreeze' and/or '-noreplacebase' AFTER the", "               '-EPI' on the command line if you do not want these options used.", "", " -parfix n v   = Fix parameter #n to be exactly at value 'v'.", " -parang n b t = Allow parameter #n to range only between 'b' and 't'.", "                 If not given, default ranges are used.", " -parini n v   = Initialize parameter #n to value 'v', but then", "                 allow the algorithm to adjust it.", "         **N.B.: Multiple '-par...' options can be used, to constrain", "                 multiple parameters.", "         **N.B.: -parini has no effect if -twopass is used, since", "                 the -twopass algorithm carries out its own search", "                 for initial parameters.", "", " -maxrot dd    = Allow maximum rotation of 'dd' degrees.  Equivalent", "                 to '-parang 4 -dd dd -parang 5 -dd dd -parang 6 -dd dd'", "                 [Default=30 degrees]", " -maxshf dd    = Allow maximum shift of 'dd' millimeters.  Equivalent", "                 to '-parang 1 -dd dd -parang 2 -dd dd -parang 3 -dd dd'", "                 [Default=32% of the size of the base image]", "         **N.B.: This max shift setting is relative to the center-of-mass", "                 shift, if the '-cmass' option is used.", " -maxscl dd    = Allow maximum scaling factor to be 'dd'.  Equivalent", "                 to '-parang 7 1/dd dd -parang 8 1/dd dd -paran2 9 1/dd dd'", "                 [Default=1.2=image can go up or down 20% in size]", " -maxshr dd    = Allow maximum shearing factor to be 'dd'. Equivalent", "                 to '-parang 10 -dd dd -parang 11 -dd dd -parang 12 -dd dd'", "                 [Default=0.1111 for no good reason]", "", " NOTE: If the datasets being registered have only 1 slice, 3dAllineate", "       will automatically fix the 6 out-of-plane motion parameters to", "       their 'do nothing' values, so you don't have to specify '-parfix'.", "", " -master mmm = Write the output dataset on the same grid as dataset", "               'mmm'.  If this option is NOT given, the base dataset", "               is the master.", "       **N.B.: 3dAllineate transforms the source dataset to be 'similar'", "               to the base image.  Therefore, the coordinate system", "               of the master dataset is interpreted as being in the", "               reference system of the base image.  It is thus vital", "               that these finite 3D volumes overlap, or you will lose data!", "       **N.B.: If 'mmm' is the string 'SOURCE', then the source dataset", "               is used as the master for the output dataset grid.", "               You can also use 'BASE', which is of course the default.", "", " -mast_dxyz del = Write the output dataset using grid spacings of", "  *OR*            'del' mm.  If this option is NOT given, then the", " -newgrid del     grid spacings in the master dataset will be used.", "                  This option is useful when registering low resolution", "                  data (e.g., EPI time series) to high resolution", "                  datasets (e.g., MPRAGE) where you don't want to", "                  consume vast amounts of disk space interpolating", "                  the low resolution data to some artificially fine", "                  (and meaningless) spatial grid.", "", "----------------------------------------------", "DEFINITION OF AFFINE TRANSFORMATION PARAMETERS", "----------------------------------------------", "The 3x3 spatial transformation matrix is calculated as [S][D][U],", "where [S] is the shear matrix,", "      [D] is the scaling matrix, and", "      [U] is the rotation (proper orthogonal) matrix.", "Thes matrices are specified in DICOM-ordered (x=-R+L,y=-A+P,z=-I+S)", "coordinates as:", "", "  [U] = [Rotate_y(param#6)] [Rotate_x(param#5)] [Rotate_z(param #4)]", "        (angles are in degrees)", "", "  [D] = diag( param#7 , param#8 , param#9 )", "", "        [    1        0     0 ]        [ 1 param#10 param#11 ]", "  [S] = [ param#10    1     0 ]   OR   [ 0    1     param#12 ]", "        [ param#11 param#12 1 ]        [ 0    0        1     ]", "", "The shift vector comprises parameters #1, #2, and #3.", "", "The goal of the program is to find the warp parameters such that", "   I([x]_warped) 'is similar to' J([x]_in)", "as closely as possible in some sense of 'similar', where J(x) is the", "base image, and I(x) is the source image.", "", "Using '-parfix', you can specify that some of these parameters", "are fixed.  For example, '-shift_rotate_scale' is equivalent", "'-affine_general -parfix 10 0 -parfix 11 0 -parfix 12 0'.", "Don't even think of using the '-parfix' option unless you grok", "this example!", "", "----------- Special Note for the '-EPI' Option's Coordinates -----------", "In this case, the parameters above are with reference to coordinates", "  x = frequency encoding direction (by default, first axis of dataset)", "  y = phase encoding direction     (by default, second axis of dataset)", "  z = slice encoding direction     (by default, third axis of dataset)", "This option lets you freeze some of the warping parameters in ways that", "make physical sense, considering how echo-planar images are acquired.", "The x- and z-scaling parameters are disabled, and shears will only affect", "the y-axis.  Thus, there will be only 9 free parameters when '-EPI' is", "used.  If desired, you can use a '-parang' option to allow the scaling", "fixed parameters to vary (put these after the '-EPI' option):", "  -parang 7 0.833 1.20     to allow x-scaling", "  -parang 9 0.833 1.20     to allow z-scaling", "You could also fix some of the other parameters, if that makes sense", "in your situation; for example, to disable out-of-slice rotations:", "  -parfix 5 0  -parfix 6 0", "and to disable out of slice translation:", "  -parfix 3 0", "NOTE WELL: If you use '-EPI', then the output warp parameters (e.g., in", "           '-1Dparam_save') apply to the (freq,phase,slice) xyz coordinates,", "           NOT to the DICOM xyz coordinates, so equivalent transformations", "           will be expressed with different sets of parameters entirely", "           than if you don't use '-EPI'!  This comment does NOT apply", "           to the output of '-1Dmatrix_save', since that matrix is", "           defined relative to the RAI (DICOM) spatial coordinates.", "", "*********** CHANGING THE ORDER OF MATRIX APPLICATION ***********", "", "  -SDU or -SUD }= Set the order of the matrix multiplication", "  -DSU or -DUS }= for the affine transformations:", "  -USD or -UDS }=   S = triangular shear (params #10-12)", "                    D = diagonal scaling matrix (params #7-9)", "                    U = rotation matrix (params #4-6)", "                  Default order is '-SDU', which means that", "                  the U matrix is applied first, then the", "                  D matrix, then the S matrix.", "", "  -Supper      }= Set the S matrix to be upper or lower", "  -Slower      }= triangular [Default=lower triangular]", "", "  -ashift OR   }= Apply the shift parameters (#1-3) after OR", "  -bshift      }= before the matrix transformation. [Default=after]", "", "            ==================================================", "        ===== RWCox - September 2006 - Live Long and Prosper =====", "            ==================================================", "", "         ********************************************************", "        *** From Webster's Dictionary: Allineate == 'to align' ***", "         ********************************************************", "", "===========================================================================", "                       FORMERLY SECRET HIDDEN OPTIONS", "---------------------------------------------------------------------------", "        ** N.B.: Most of these are experimental! [permanent beta] **", "===========================================================================", "", " -num_rtb n  = At the beginning of the fine pass, the best set of results", "               from the coarse pass are 'refined' a little by further", "               optimization, before the single best one is chosen for", "               for the final fine optimization.", "              * This option sets the maximum number of cost functional", "                evaluations to be used (for each set of parameters)", "                in this step.", "              * The default is 99; a larger value will take more CPU", "                time but may give more robust results.", "              * If you want to skip this step entirely, use '-num_rtb 0'.", "                then, the best of the coarse pass results is taken", "                straight to the final optimization passes.", "       **N.B.: If you use '-VERB', you will see that one extra case", "               is involved in this initial fine refinement step; that", "               case is starting with the identity transformation, which", "               helps insure against the chance that the coarse pass", "               optimizations ran totally amok.", " -nocast     = By default, parameter vectors that are too close to the", "               best one are cast out at the end of the coarse pass", "               refinement process. Use this option if you want to keep", "               them all for the fine resolution pass.", " -norefinal  = Do NOT re-start the fine iteration step after it", "               has converged.  The default is to re-start it, which", "               usually results in a small improvement to the result", "               (at the cost of CPU time).  This re-start step is an", "               an attempt to avoid a local minimum trap.  It is usually", "               not necessary, but sometimes helps.", "", " -realaxes   = Use the 'real' axes stored in the dataset headers, if they", "               conflict with the default axes.  [For Jedi AFNI Masters only!]", "", " -savehist sss = Save start and final 2D histograms as PGM", "                 files, with prefix 'sss' (cost: cr mi nmi hel).", "                * if filename contains 'FF', floats is written", "                * these are the weighted histograms!", "                * -savehist will also save histogram files when", "                  the -allcost evaluations takes place", "                * this option is mostly useless unless '-histbin' is", "                  also used", " -median       = Smooth with median filter instead of Gaussian blur.", "                 (Somewhat slower, and not obviously useful.)", " -powell m a   = Set the Powell NEWUOA dimensional parameters to", "                 'm' and 'a' (cf. source code in powell_int.c).", "                 The number of points used for approximating the", "                 cost functional is m*N+a, where N is the number", "                 of parameters being optimized.  The default values", "                 are m=2 and a=3.  Larger values will probably slow", "                 the program down for no good reason.  The smallest", "                 allowed values are 1.", " -target ttt   = Same as '-source ttt'.  In the earliest versions,", "                 what I now call the 'source' dataset was called the", "                 'target' dataset:", "                    Try to remember the kind of September (2006)", "                    When life was slow and oh so mellow", "                    Try to remember the kind of September", "                    When grass was green and source was target.", " -Xwarp       =} Change the warp/matrix setup so that only the x-, y-, or z-", " -Ywarp       =} axis is stretched & sheared.  Useful for EPI, where 'X',", " -Zwarp       =} 'Y', or 'Z' corresponds to the phase encoding direction.", " -FPS fps      = Generalizes -EPI to arbitrary permutation of directions.", " -histpow pp   = By default, the number of bins in the histogram used", "                 for calculating the Hellinger, Mutual Information, and", "                 Correlation Ratio statistics is n^(1/3), where n is", "                 the number of data points.  You can change that exponent", "                 to 'pp' with this option.", " -histbin nn   = Or you can just set the number of bins directly to 'nn'.", " -eqbin   nn   = Use equalized marginal histograms with 'nn' bins.", " -clbin   nn   = Use 'nn' equal-spaced bins except for the bot and top,", "                 which will be clipped (thus the 'cl').  If nn is 0, the", "                 program will pick the number of bins for you.", "                 **N.B.: '-clbin 0' is now the default [25 Jul 2007];", "                         if you want the old all-equal-spaced bins, use", "                         '-histbin 0'.", "                 **N.B.: '-clbin' only works when the datasets are", "                         non-negative; any negative voxels in either", "                         the input or source volumes will force a switch", "                         to all equal-spaced bins.", " -wtmrad  mm   = Set autoweight/mask median filter radius to 'mm' voxels.", " -wtgrad  gg   = Set autoweight/mask Gaussian filter radius to 'gg' voxels.", " -nmsetup nn   = Use 'nn' points for the setup matching [default=98756]", " -ignout       = Ignore voxels outside the warped source dataset.", "", " -blok bbb     = Blok definition for the 'lp?' (Local Pearson) cost", "                 functions: 'bbb' is one of", "                   'BALL(r)' or 'CUBE(r)' or 'RHDD(r)' or 'TOHD(r)'", "                 corresponding to", "                   spheres or cubes or rhombic dodecahedra or", "                   truncated octahedra", "                 where 'r' is the size parameter in mm.", "                 [Default is 'RHDD(6.54321)' (rhombic dodecahedron)]", "", " -allcost        = Compute ALL available cost functionals and print them", "                   at various points.", " -allcostX       = Compute and print ALL available cost functionals for the", "                   un-warped inputs, and then quit.", " -allcostX1D p q = Compute ALL available cost functionals for the set of", "                   parameters given in the 1D file 'p' (12 values per row),", "                   write them to the 1D file 'q', then exit. (For you, Zman)", "                  * N.B.: If -fineblur is used, that amount of smoothing", "                          will be applied prior to the -allcostX evaluations.", "                          The parameters are the rotation, shift, scale,", "                          and shear values, not the affine transformation", "                          matrix. An identity matrix could be provided as", "                          \"0 0 0  0 0 0  1 1 1  0 0 0\" for instance or by", "                          using the word \"IDENTITY\"", "", "===========================================================================", "", "Modifying '-final wsinc5'", "-------------------------", " * The windowed (tapered) sinc function interpolation can be modified", "     by several environment variables.  This is expert-level stuff, and", "     you should understand what you are doing if you use these options.", "     The simplest way to use these would be on the command line, as in", "       -DAFNI_WSINC5_RADIUS=9 -DAFNI_WSINC5_TAPERFUN=Hamming", "", " * AFNI_WSINC5_TAPERFUN lets you choose the taper function.", "     The default taper function is the minimum sidelobe 3-term cosine:", "       0.4243801 + 0.4973406*cos(PI*x) + 0.0782793*cos(2*PI*x)", "     If you set this environment variable to 'Hamming', then the", "     minimum sidelobe 2-term cosine will be used instead:", "       0.53836 + 0.46164*cos(PI*x)", "     Here, 'x' is between 0 and 1, where x=0 is the center of the", "     interpolation mask and x=1 is the outer edge.", " ++  Unfortunately, the 3-term cosine doesn't have a catchy name; you can", "       find it (and many other) taper functions described in the paper", "         AH Nuttall, Some Windows with Very Good Sidelobe Behavior.", "         IEEE Trans. ASSP, 29:84-91 (1981).", "       In particular, see Fig.14 and Eq.36 in this paper.", "", " * AFNI_WSINC5_TAPERCUT lets you choose the start 'x' point for tapering:", "     This value should be between 0 and 0.8; for example, 0 means to taper", "     all the way from x=0 to x=1 (maximum tapering).  The default value", "     is 0.  Setting TAPERCUT to 0.5 (say) means only to taper from x=0.5", "     to x=1; thus, a larger value means that fewer points are tapered", "     inside the interpolation mask.", "", " * AFNI_WSINC5_RADIUS lets you choose the radius of the tapering window", "     (i.e., the interpolation mask region).  This value is an integer", "     between 3 and 21.  The default value is 5 (which used to be the", "     ONLY value, thus 'wsinc5').  RADIUS is measured in voxels, not mm.", "", " * AFNI_WSINC5_SPHERICAL lets you choose the shape of the mask region.", "     If you set this value to 'Yes', then the interpolation mask will be", "     spherical; otherwise, it defaults to cubical.", "", " * The Hamming taper function is a little faster than the 3-term function,", "     but will have a little more Gibbs phenomenon.", " * A larger TAPERCUT will give a little more Gibbs phenomenon; compute", "     speed won't change much with this parameter.", " * Compute time goes up with (at least) the 3rd power of the RADIUS; setting", "     RADIUS to 21 will be VERY slow.", " * Visually, RADIUS=3 is similar to quintic interpolation.  Increasing", "     RADIUS makes the interpolated images look sharper and more well-", "     defined.  However, values of RADIUS greater than or equal to 7 appear", "     (to Zhark's eagle eye) to be almost identical.  If you really care,", "     you'll have to experiment with this parameter yourself.", " * A spherical mask is also VERY slow, since the cubical mask allows", "     evaluation as a tensor product.  There is really no good reason", "     to use a spherical mask; I only put it in for experimental purposes.", "** For most users, there is NO reason to ever use these environment variables", "     to modify wsinc5.  You should only do this kind of thing if you have a", "     good and articulable reason!  (Or if you really like to screw around.)", "** The wsinc5 interpolation function is parallelized using OpenMP, which", "     makes its usage moderately tolerable.", "", "===========================================================================", "", "Hidden experimental cost functionals:", "-------------------------------------", "   sp   *OR*  spearman        = Spearman [rank] Correlation", "   je   *OR*  jointentropy    = Joint Entropy [H(b,s)]", "   lss  *OR*  signedPcor      = Signed Pearson Correlation", "   lpc  *OR*  localPcorSigned = Local Pearson Correlation Signed", "   lpa  *OR*  localPcorAbs    = Local Pearson Correlation Abs", "   lpc+ *OR*  localPcor+Others= Local Pearson Signed + Others", "   ncd  *OR*  NormCompDist    = Normalized Compression Distance", "", "Notes for the new [Feb 2010] lpc+ cost functional:", "--------------------------------------------------", " * The cost functional named 'lpc+' is a combination of several others:", "     lpc + hel*0.4 + crA*0.4 + nmi*0.2 + mi*0.2 + ov*0.4", "   ++ 'hel', 'crA', 'nmi', and 'mi' are the histogram-based cost", "      functionals also available as standalone options.", "   ++ 'ov' is a measure of the overlap of the automasks of the base and", "      source volumes; ov is not available as a standalone option.", " * The purpose of lpc+ is to avoid situations where the pure lpc cost", "   goes wild; this especially happens if '-source_automask' isn't used.", "   ++ Even with lpc+, you should use '-source_automask+2' (say) to be safe.", " * You can alter the weighting of the extra functionals by giving the", "   option in the form (for example)", "     '-lpc+hel*0.5+nmi*0+mi*0+crA*1.0+ov*0.5'", " * The quotes are needed to prevent the shell from wild-card expanding", "   the '*' character.", "   --> You can now use ':' in place of '*' to avoid this wildcard problem:", "         -lpc+hel:0.5+nmi:0+mi:0+crA:1+ov:0.5+ZZ", " * Notice the weight factors FOLLOW the name of the extra functionals.", "   ++ If you want a weight to be 0 or 1, you have to provide for that", "      explicitly -- if you leave a weight off, then it will get its", "      default value!", "   ++ The order of the weight factor names is unimportant here:", "        '-lpc+hel*0.5+nmi*0.8' == '-lpc+nmi*0.8+hel*0.5'", " * Only the 5 functionals listed (hel,crA,nmi,mi,ov) can be used in '-lpc+'.", " * In addition, if you want the initial alignments to be with '-lpc+' and", "   then finish the Final alignment with pure '-lpc', you can indicate this", "   by putting 'ZZ' somewhere in the option string, as in '-lpc+ZZ'.", " * This stuff should be considered really experimental at this moment!", "", "Cost functional descriptions (for use with -allcost output):", "------------------------------------------------------------", "   ls  :: 1 - abs(Pearson correlation coefficient)", "   sp  :: 1 - abs(Spearman correlation coefficient)", "   mi  :: - Mutual Information = H(base,source)-H(base)-H(source)", "   crM :: 1 - abs[ CR(base,source) * CR(source,base) ]", "   nmi :: 1/Normalized MI = H(base,source)/[H(base)+H(source)]", "   je  :: H(base,source) = joint entropy of image pair", "   hel :: - Hellinger distance(base,source)", "   crA :: 1 - abs[ CR(base,source) + CR(source,base) ]", "   crU :: CR(source,base) = Var(source|base) / Var(source)", "   lss :: Pearson correlation coefficient between image pair", "   lpc :: nonlinear average of Pearson cc over local neighborhoods", "   lpa :: 1 - abs(lpc)", "   lpc+:: lpc + hel + mi + nmi + crA + overlap", "   ncd :: mutual compressibility (via zlib) -- doesn't work yet", "", " * N.B.: Some cost functional values (as printed out above)", "   are negated from their theoretical descriptions (e.g., 'hel')", "   so that the best image alignment will be found when the cost", "   is minimized.  See the descriptions above and the references", "   below for more details for each functional.", "", " * For more information about the 'lpc' functional, see", "     ZS Saad, DR Glen, G Chen, MS Beauchamp, R Desai, RW Cox.", "       A new method for improving functional-to-structural", "       MRI alignment using local Pearson correlation.", "       NeuroImage 44: 839-848, 2009.", "     http://dx.doi.org/10.1016/j.neuroimage.2008.09.037", "     https://afni.nimh.nih.gov/sscc/rwcox/papers/LocalPearson2009.pdf", "   The '-blok' option can be used to control the regions", "   (size and shape) used to compute the local correlations.", " *** Using the 'lpc' functional wisely requires the use of", "     a proper weight volume.  We HIGHLY recommend you use", "     the align_epi_anat.py script if you want to use this", "     cost functional!  Otherwise, you are likely to get", "     less than optimal results (and then swear at us unjustly).", "", " * For more information about the 'cr' functionals, see", "     http://en.wikipedia.org/wiki/Correlation_ratio", "   Note that CR(x,y) is not the same as CR(y,x), which", "   is why there are symmetrized versions of it available.", "", " * For more information about the 'mi', 'nmi', and 'je'", "   cost functionals, see", "     http://en.wikipedia.org/wiki/Mutual_information", "     http://en.wikipedia.org/wiki/Joint_entropy", "     http://www.cs.jhu.edu/~cis/cista/746/papers/mutual_info_survey.pdf", "", " * For more information about the 'hel' functional, see", "     http://en.wikipedia.org/wiki/Hellinger_distance", "", " * Some cost functionals (e.g., 'mi', 'cr', 'hel') are", "   computed by creating a 2D joint histogram of the", "   base and source image pair.  Various options above", "   (e.g., '-histbin', etc.) can be used to control the", "   number of bins used in the histogram on each axis.", "   (If you care to control the program in such detail!)", "", " * Minimization of the chosen cost functional is done via", "   the NEWUOA software, described in detail in", "     MJD Powell. 'The NEWUOA software for unconstrained", "       optimization without derivatives.' In: GD Pillo,", "       M Roma (Eds), Large-Scale Nonlinear Optimization.", "       Springer, 2006.", "     http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2004_08.pdf", "", "===========================================================================", "", " -nwarp type = Experimental nonlinear warping:", "", "              ***** Note that these '-nwarp' options are superseded  *****", "              ***** by the AFNI program 3dQwarp,  which does a more  *****", "              ***** accurate and better and job of nonlinear warping *****", "              ***** ------ Zhark the Warper ------ July 2013 ------- *****", "", "              * At present, the only 'type' is 'bilinear',", "                as in 3dWarpDrive, with 39 parameters.", "              * I plan to implement more complicated nonlinear", "                warps in the future, someday ....", "              * -nwarp can only be applied to a source dataset", "                that has a single sub-brick!", "              * -1Dparam_save and -1Dparam_apply work with", "                bilinear warps; see the Notes for more information.", "        ==>>*** Nov 2010: I have now added the following polynomial", "                warps: 'cubic', 'quintic', 'heptic', 'nonic' (using", "                3rd, 5th, 7th, and 9th order Legendre polynomials); e.g.,", "                   -nwarp heptic", "              * These are the nonlinear warps that I now am supporting.", "              * Or you can call them 'poly3', 'poly5', 'poly7', and 'poly9',", "                  for simplicity and non-Hellenistic clarity.", "              * These names are not case sensitive: 'nonic' == 'Nonic', etc.", "              * Higher and higher order polynomials will take longer and longer", "                to run!", "              * If you wish to apply a nonlinear warp, you have to supply", "                a parameter file with -1Dparam_apply and also specify the", "                warp type with -nwarp.  The number of parameters in the", "                file (per line) must match the warp type:", "                   bilinear =  43   [for all nonlinear warps, the final]", "                   cubic    =  64   [4 'parameters' are fixed values to]", "                   quintic  = 172   [normalize the coordinates to -1..1]", "                   heptic   = 364   [for the nonlinear warp functions. ]", "                   nonic    = 664", "                In all these cases, the first 12 parameters are the", "                affine parameters (shifts, rotations, etc.), and the", "                remaining parameters define the nonlinear part of the warp", "                (polynomial coefficients); thus, the number of nonlinear", "                parameters over which the optimization takes place is", "                the number in the table above minus 16.", "               * The actual polynomial functions used are products of", "                 Legendre polynomials, but the symbolic names used in", "                 the header line in the '-1Dparam_save' output just", "                 express the polynomial degree involved; for example,", "                      quint:x^2*z^3:z", "                 is the name given to the polynomial warp basis function", "                 whose highest power of x is 2, is independent of y, and", "                 whose highest power of z is 3; the 'quint' indicates that", "                 this was used in '-nwarp quintic'; the final ':z' signifies", "                 that this function was for deformations in the (DICOM)", "                 z-direction (+z == Superior).", "        ==>>*** You can further control the form of the polynomial warps", "                (but not the bilinear warp!) by restricting their degrees", "                of freedom in 2 different ways.", "                ++ You can remove the freedom to have the nonlinear", "                   deformation move along the DICOM x, y, and/or z axes.", "                ++ You can remove the dependence of the nonlinear", "                   deformation on the DICOM x, y, and/or z coordinates.", "                ++ To illustrate with the six second order polynomials:", "                      p2_xx(x,y,z) = x*x  p2_xy(x,y,z) = x*y", "                      p2_xz(x,y,z) = x*z  p2_yy(x,y,z) = y*y", "                      p2_yz(x,y,z) = y*z  p2_zz(x,y,z) = z*z", "                   Unrestricted, there are 18 parameters associated with", "                   these polynomials, one for each direction of motion (x,y,z)", "                   * If you remove the freedom of the nonlinear warp to move", "                     data in the z-direction (say), then there would be 12", "                     parameters left.", "                   * If you instead remove the freedom of the nonlinear warp", "                     to depend on the z-coordinate, you would be left with", "                     3 basis functions (p2_xz, p2_yz, and p2_zz would be", "                     eliminated), each of which would have x-motion, y-motion,", "                     and z-motion parameters, so there would be 9 parameters.", "                ++ To fix motion along the x-direction, use the option", "                   '-nwarp_fixmotX' (and '-nwarp_fixmotY' and '-nwarp_fixmotZ).", "                ++ To fix dependence of the polynomial warp on the x-coordinate,", "                   use the option '-nwarp_fixdepX' (et cetera).", "                ++ These coordinate labels in the options (X Y Z) refer to the", "                   DICOM directions (X=R-L, Y=A-P, Z=I-S).  If you would rather", "                   fix things along the dataset storage axes, you can use", "                   the symbols I J K to indicate the fastest to slowest varying", "                   array dimensions (e.g., '-nwarp_fixdepK').", "                   * Mixing up the X Y Z and I J K forms of parameter freezing", "                     (e.g., '-nwarp_fixmotX -nwarp_fixmotJ') may cause trouble!", "                ++ If you input a 2D dataset (a single slice) to be registered", "                   with '-nwarp', the program automatically assumes '-nwarp_fixmotK'", "                   and '-nwarp_fixdepK' so there are no out-of-plane parameters", "                   or dependence.  The number of nonlinear parameters is then:", "                     2D: cubic = 14 ; quintic =  36 ; heptic =  66 ; nonic = 104.", "                     3D: cubic = 48 ; quintic = 156 ; heptic = 348 ; nonic = 648.", "                     [ n-th order: 2D = (n+4)*(n-1) ; 3D = (n*n+7*n+18)*(n-1)/2 ]", "                ++ Note that these '-nwarp_fix' options have no effect on the", "                   affine part of the warp -- if you want to constrain that as", "                   well, you'll have to use the '-parfix' option.", "                   * However, for 2D images, the affine part will automatically", "                     be restricted to in-plane (6 parameter) 'motions'.", "                ++ If you save the warp parameters (with '-1Dparam_save') when", "                   doing 2D registration, all the parameters will be saved, even", "                   the large number of them that are fixed to zero. You can use", "                   '1dcat -nonfixed' to remove these columns from the 1D file if", "                   you want to further process the varying parameters (e.g., 1dsvd).", "              **++ The mapping from I J K to X Y Z (DICOM coordinates), where the", "                   '-nwarp_fix' constraints are actually applied, is very simple:", "                   given the command to fix K (say), the coordinate X, or Y, or Z", "                   whose direction most closely aligns with the dataset K grid", "                   direction is chosen.  Thus, for coronal images, K is in the A-P", "                   direction, so '-nwarp_fixmotK' is translated to '-nwarp_fixmotY'.", "                   * This simplicity means that using the '-nwarp_fix' commands on", "                     oblique datasets is problematic.  Perhaps it would work in", "                     combination with the '-EPI' option, but that has not been tested.", "", "-nwarp NOTES:", "-------------", "* -nwarp is slow - reeeaaallll slow - use it with OpenMP!", "* Check the results to make sure the optimizer didn't run amok!", "   (You should ALWAYS do this with any registration software.)", "* For the nonlinear warps, the largest coefficient allowed is", "   set to 0.10 by default.  If you wish to change this, use an", "   option like '-nwarp_parmax 0.05' (to make the allowable amount", "   of nonlinear deformation half the default).", "  ++ N.B.: Increasing the maximum past 0.10 may give very bad results!!", "* If you use -1Dparam_save, then you can apply the nonlinear", "   warp to another dataset using -1Dparam_apply in a later", "   3dAllineate run. To do so, use '-nwarp xxx' in both runs", "   , so that the program knows what the extra parameters in", "   the file are to be used for.", "  ++ Bilinear: 43 values are saved in 1 row of the param file.", "  ++ The first 12 are the affine parameters", "  ++ The next 27 are the D1,D2,D3 matrix parameters (cf. infra).", "  ++ The final 'extra' 4 values are used to specify", "      the center of coordinates (vector Xc below), and a", "      pre-computed scaling factor applied to parameters #13..39.", "  ++ For polynomial warps, a similar format is used (mutatis mutandis).", "* The option '-nwarp_save sss' lets you save a 3D dataset of the", "  the displacement field used to create the output dataset.  This", "  dataset can be used in program 3dNwarpApply to warp other datasets.", "  ++ If the warp is symbolized by x -> w(x) [here, x is a DICOM 3-vector],", "     then the '-nwarp_save' dataset contains w(x)-x; that is, it contains", "     the warp displacement of each grid point from its grid location.", "  ++ Also see program 3dNwarpCalc for other things you can do with this file:", "       warp inversion, catenation, square root, ...", "", "* Bilinear warp formula:", "   Xout = inv[ I + {D1 (Xin-Xc) | D2 (Xin-Xc) | D3 (Xin-Xc)} ] [ A Xin ]", "  where Xin  = input vector  (base dataset coordinates)", "        Xout = output vector (source dataset coordinates)", "        Xc   = center of coordinates used for nonlinearity", "               (will be the center of the base dataset volume)", "        A    = matrix representing affine transformation (12 params)", "        I    = 3x3 identity matrix", "    D1,D2,D3 = three 3x3 matrices (the 27 'new' parameters)", "               * when all 27 parameters == 0, warp is purely affine", "     {P|Q|R} = 3x3 matrix formed by adjoining the 3-vectors P,Q,R", "    inv[...] = inverse 3x3 matrix of stuff inside '[...]'", "* The inverse of a bilinear transformation is another bilinear", "   transformation.  Someday, I may write a program that will let", "   you compute that inverse transformation, so you can use it for", "   some cunning and devious purpose.", "* If you expand the inv[...] part of the above formula in a 1st", "   order Taylor series, you'll see that a bilinear warp is basically", "   a quadratic warp, with the additional feature that its inverse", "   is directly computable (unlike a pure quadratic warp).", "* 'bilinearD' means the matrices D1, D2, and D3 with be constrained", "  to be diagonal (a total of 9 nonzero values), rather than full", "  (a total of 27 nonzero values).  This option is much faster.", "* Is '-nwarp bilinear' useful?  Try it and tell me!", "* Unlike a bilinear warp, the polynomial warps cannot be exactly", "  inverted.  At some point, I'll write a program to compute an", "  approximate inverse, if there is enough clamor for such a toy.", "", "===========================================================================", "", " =========================================================================", "* This binary version of 3dAllineate is compiled using OpenMP, a semi-", "   automatic parallelizer software toolkit, which splits the work across", "   multiple CPUs/cores on the same shared memory computer.", "* OpenMP is NOT like MPI -- it does not work with CPUs connected only", "   by a network (e.g., OpenMP doesn't work with 'cluster' setups).", "* For implementation and compilation details, please see", "   https://afni.nimh.nih.gov/pub/dist/doc/misc/OpenMP.html", "* The number of CPU threads used will default to the maximum number on", "   your system. You can control this value by setting environment variable", "   OMP_NUM_THREADS to some smaller value (including 1).", "* Un-setting OMP_NUM_THREADS resets OpenMP back to its default state of", "   using all CPUs available.", "   ++ However, on some systems, it seems to be necessary to set variable", "      OMP_NUM_THREADS explicitly, or you only get one CPU.", "   ++ On other systems with many CPUS, you probably want to limit the CPU", "      count, since using more than (say) 16 threads is probably useless.", "* You must set OMP_NUM_THREADS in the shell BEFORE running the program,", "   since OpenMP queries this variable BEFORE the program actually starts.", "   ++ You can't usefully set this variable in your ~/.afnirc file or on the", "      command line with the '-D' option.", "* How many threads are useful? That varies with the program, and how well", "   it was coded. You'll have to experiment on your own systems!", "* The number of CPUs on this particular computer system is ...... 2.", "* The maximum number of CPUs that will be used is now set to .... 2.", "* OpenMP may or may not speed up the program significantly.  Limited", "   tests show that it provides some benefit, particularly when using", "   the more complicated interpolation methods (e.g., '-cubic' and/or", "   '-final wsinc5'), for up to 3-4 CPU threads.", "* But the speedup is definitely not linear in the number of threads, alas.", "   Probably because my parallelization efforts were pretty limited.", " =========================================================================", "", "++ Compile date = May 25 2018 {AFNI_18.1.18:macosx_10.7_Intel_64}", ""], "params": [{"param": "-base", "line_start": 51, "length": 5, "param_range": [3185, 3190], "help": "Set the base dataset to be the #0 sub-brick of 'bbb'.\n               If no -base option is given, then the base volume is\n               taken to be the #0 sub-brick of the source dataset.\n               (Base must be stored as floats, shorts, or bytes.)", "help_range": [3199, 3453]}, {"param": "-source", "line_start": 56, "length": 2, "param_range": [3456, 3463], "help": "Read the source dataset from 'ttt'.  If no -source\n   *OR*        (or -input) option is given, then the source dataset", "help_range": [3470, 3588]}, {"param": "-input", "line_start": 58, "length": 28, "param_range": [3590, 3596], "help": " -input ttt    is the last argument on the command line.\n               (Source must be stored as floats, shorts, or bytes.)\n            ** 3dAllineate can register 2D datasets (single slice),\n               but both the base and source must be 2D -- you cannot\n               use this program to register a 2D slice into a 3D volume!\n            ** See the script @2dwarper.Allin for an example of using\n               3dAllineate to do slice-by-slice nonlinear warping to\n               align 3D volumes distorted by time-dependent magnetic\n               field inhomogeneities.\n\n ** NOTA BENE: The base and source dataset do NOT have to be defined **\n ** [that's]   on the same 3D grids; the alignment process uses the  **\n ** [Latin ]   coordinate systems defined in the dataset headers to  **\n ** [  for ]   make the match between spatial locations, rather than **\n ** [ NOTE ]   matching the 2 datasets on a voxel-by-voxel basis     **\n ** [ WELL ]   (as 3dvolreg and 3dWarpDrive do).                     **\n **       -->> However, this coordinate-based matching requires that **\n **            image volumes be defined on roughly the same patch of **\n **            of (x,y,z) space, in order to find a decent starting  **\n **            point for the transformation.  You might need to use  **\n **            the script @Align_Centers to do this, if the 3D       **\n **            spaces occupied by the images do not overlap much.    **\n **       -->> Or the '-cmass' option to this program might be       **\n **            sufficient to solve this problem, maybe, with luck.   **\n **            (Another reason why you should use align_epi_anat.py) **\n **       -->> If the coordinate system in the dataset headers is    **\n **            WRONG, then 3dAllineate will probably not work well!  **", "help_range": [3589, 5394]}, {"param": "-prefix", "line_start": 86, "length": 2, "param_range": [5397, 5404], "help": "Output the resulting dataset to file 'ppp'.  If this\n   *OR*        option is NOT given, no dataset will be output!  The", "help_range": [5411, 5531]}, {"param": "-out", "line_start": 88, "length": 9, "param_range": [5533, 5537], "help": " -out ppp      transformation matrix to align the source to the base will\n               be estimated, but not applied.  You can save the matrix\n               for later use using the '-1Dmatrix_save' option.\n        *N.B.: By default, the new dataset is computed on the grid of the\n                base dataset; see the '-master' and/or the '-mast_dxyz'\n                options to change this grid.\n        *N.B.: If 'ppp' is 'NULL', then no output dataset will be produced.\n                This option is for compatibility with 3dvolreg.", "help_range": [5532, 6071]}, {"param": "-floatize", "line_start": 97, "length": 1, "param_range": [6074, 6083], "help": "Write result dataset as floats.  Internal calculations", "help_range": [6088, 6142]}, {"param": "-float", "line_start": 98, "length": 8, "param_range": [6144, 6150], "help": " -float        are all done on float copies of the input datasets.\n               [Default=convert output dataset to data format of  ]\n               [        source dataset; if the source dataset was ]\n               [        shorts with a scale factor, then the new  ]\n               [        dataset will get a scale factor as well;  ]\n               [        if the source dataset was shorts with no  ]\n               [        scale factor, the result will be unscaled.]", "help_range": [6143, 6617]}, {"param": "-1Dparam_save", "line_start": 106, "length": 14, "param_range": [6620, 6633], "help": "Save the warp parameters in ASCII (.1D) format into\n                      file 'ff' (1 row per sub-brick in source).\n                    * A historical synonym for this option is '-1Dfile'.\n                    * At the top of the saved 1D file is a #comment line\n                      listing the names of the parameters; those parameters\n                      that are fixed (e.g., via '-parfix') will be marked\n                      by having their symbolic names end in the '$' character.\n                      You can use '1dcat -nonfixed' to remove these columns\n                      from the 1D file if you just want to further process the\n                      varying parameters somehow (e.g., 1dsvd).\n                    * However, the '-1Dparam_apply' option requires the\n                      full list of parameters, including those that were\n                      fixed, in order to work properly!", "help_range": [6641, 7552]}, {"param": "-1Dparam_apply", "line_start": 120, "length": 41, "param_range": [7555, 7569], "help": "Read warp parameters from file 'aa', apply them to \n                      the source dataset, and produce a new dataset.\n                      (Must also use the '-prefix' option for this to work!  )\n                      (In this mode of operation, there is no optimization of)\n                      (the cost functional by changing the warp parameters;  )\n                      (previously computed parameters are applied directly.  )\n               *N.B.: A historical synonym for this is '-1Dapply'.\n               *N.B.: If you use -1Dparam_apply, you may also want to use\n                       -master to control the grid on which the new\n                       dataset is written -- the base dataset from the\n                       original 3dAllineate run would be a good possibility.\n                       Otherwise, the new dataset will be written out on the\n                       3D grid coverage of the source dataset, and this\n                       might result in clipping off part of the image.\n               *N.B.: Each row in the 'aa' file contains the parameters for\n                       transforming one sub-brick in the source dataset.\n                       If there are more sub-bricks in the source dataset\n                       than there are rows in the 'aa' file, then the last\n                       row is used repeatedly.\n               *N.B.: A trick to use 3dAllineate to resample a dataset to\n                       a finer grid spacing:\n                         3dAllineate -input dataset+orig         \\\n                                     -master template+orig       \\\n                                     -prefix newdataset          \\\n                                     -final wsinc5               \\\n                                     -1Dparam_apply '1D: 12@0'\\'  \n                       Here, the identity transformation is specified\n                       by giving all 12 affine parameters as 0 (note\n                       the extra \\' at the end of the '1D: 12@0' input!).\n                     ** You can also use the word 'IDENTITY' in place of\n                        '1D: 12@0'\\' (to indicate the identity transformation).\n              **N.B.: Some expert options for modifying how the wsinc5\n                       method works are described far below, if you use\n                       '-HELP' instead of '-help'.\n            ****N.B.: The interpolation method used to produce a dataset\n                       is always given via the '-final' option, NOT via\n                       '-interp'.  If you forget this and use '-interp'\n                       along with one of the 'apply' options, this program\n                       will chastise you (gently) and change '-final'\n                       to match what the '-interp' input.", "help_range": [7576, 10368]}, {"param": "-1Dmatrix_save", "line_start": 161, "length": 19, "param_range": [10371, 10385], "help": "Save the transformation matrix for each sub-brick into\n                      file 'ff' (1 row per sub-brick in the source dataset).\n                      If 'ff' does NOT end in '.1D', then the program will\n                      append '.aff12.1D' to 'ff' to make the output filename.\n               *N.B.: This matrix is the coordinate transformation from base\n                       to source DICOM coordinates. In other terms:\n                          Xin = Xsource = M Xout = M Xbase\n                                   or\n                          Xout = Xbase = inv(M) Xin = inv(M) Xsource\n                       where Xin or Xsource is the 4x1 coordinates of a\n                       location in the input volume. Xout is the \n                       coordinate of that same location in the output volume.\n                       Xbase is the coordinate of the corresponding location\n                       in the base dataset. M is ff augmented by a 4th row of\n                       [0 0 0 1], X. is an augmented column vector [x,y,z,1]'\n                       To get the inverse matrix inv(M)\n                       (source to base), use the cat_matvec program, as in\n                         cat_matvec fred.aff12.1D -I", "help_range": [10392, 11620]}, {"param": "-1Dmatrix_apply", "line_start": 180, "length": 21, "param_range": [11623, 11638], "help": "Use the matrices in file 'aa' to define the spatial\n                      transformations to be applied.  Also see program\n                      cat_matvec for ways to manipulate these matrix files.\n               *N.B.: You probably want to use either -base or -master\n                      with either *_apply option, so that the coordinate\n                      system that the matrix refers to is correctly loaded.\n                     ** You can also use the word 'IDENTITY' in place of a\n                        filename to indicate the identity transformation --\n                        presumably for the purpose of resampling the source\n                        dataset to a new grid.\n\n  * The -1Dmatrix_* options can be used to save and re-use the transformation *\n  * matrices.  In combination with the program cat_matvec, which can multiply *\n  * saved transformation matrices, you can also adjust these matrices to      *\n  * other alignments.                                                         *\n\n  * The script 'align_epi_anat.py' uses 3dAllineate and 3dvolreg to align EPI *\n  * datasets to T1-weighted anatomical datasets, using saved matrices between *\n  * the two programs.  This script is our currently recommended method for    *\n  * doing such intra-subject alignments.                                      *", "help_range": [11644, 12978]}, {"param": "-cost", "line_start": 201, "length": 14, "param_range": [12981, 12986], "help": "Defines the 'cost' function that defines the matching\n               between the source and the base; 'ccc' is one of\n                ls   *OR*  leastsq         = Least Squares [Pearson Correlation]\n                mi   *OR*  mutualinfo      = Mutual Information [H(b)+H(s)-H(b,s)]\n                crM  *OR*  corratio_mul    = Correlation Ratio (Symmetrized*)\n                nmi  *OR*  norm_mutualinfo = Normalized MI [H(b,s)/(H(b)+H(s))]\n                hel  *OR*  hellinger       = Hellinger metric\n                crA  *OR*  corratio_add    = Correlation Ratio (Symmetrized+)\n                crU  *OR*  corratio_uns    = Correlation Ratio (Unsym)\n               You can also specify the cost functional using an option\n               of the form '-mi' rather than '-cost mi', if you like\n               to keep things terse and cryptic (as I do).\n               [Default == '-hel' (for no good reason, but it sounds nice).]", "help_range": [12995, 13922]}, {"param": "-interp", "line_start": 215, "length": 20, "param_range": [13925, 13932], "help": "Defines interpolation method to use during matching\n               process, where 'iii' is one of\n                 NN      *OR* nearestneighbour *OR nearestneighbor\n                 linear  *OR* trilinear\n                 cubic   *OR* tricubic\n                 quintic *OR* triquintic\n               Using '-NN' instead of '-interp NN' is allowed (e.g.).\n               Note that using cubic or quintic interpolation during\n               the matching process will slow the program down a lot.\n               Use '-final' to affect the interpolation method used\n               to produce the output dataset, once the final registration\n               parameters are determined.  [Default method == 'linear'.]\n            ** N.B.: Linear interpolation is used during the coarse\n                     alignment pass; the selection here only affects\n                     the interpolation method used during the second\n                     (fine) alignment pass.\n            ** N.B.: '-interp' does NOT define the final method used\n                     to produce the output dataset as warped from the\n                     input dataset.  If you want to do that, use '-final'.", "help_range": [13939, 15111]}, {"param": "-final", "line_start": 235, "length": 23, "param_range": [15114, 15120], "help": "Defines the interpolation mode used to create the\n               output dataset.  [Default == 'cubic']\n            ** N.B.: For '-final' ONLY, you can use 'wsinc5' to specify\n                       that the final interpolation be done using a\n                       weighted sinc interpolation method.  This method\n                       is so SLOW that you aren't allowed to use it for\n                       the registration itself.\n                  ++ wsinc5 interpolation is highly accurate and should\n                       reduce the smoothing artifacts from lower\n                       order interpolation methods (which are most\n                       visible if you interpolate an EPI time series\n                       to high resolution and then make an image of\n                       the voxel-wise variance).\n                  ++ On my Intel-based Mac, it takes about 2.5 s to do\n                       wsinc5 interpolation, per 1 million voxels output.\n                       For comparison, quintic interpolation takes about\n                       0.3 s per 1 million voxels: 8 times faster than wsinc5.\n                  ++ The '5' refers to the width of the sinc interpolation\n                       weights: plus/minus 5 grid points in each direction;\n                       this is a tensor product interpolation, for speed.\n\nTECHNICAL OPTIONS (used for fine control of the program):\n=================", "help_range": [15128, 16551]}, {"param": "-nmatch", "line_start": 258, "length": 8, "param_range": [16553, 16560], "help": "Use at most 'nnn' scattered points to match the\n               datasets.  The smaller nnn is, the faster the matching\n               algorithm will run; however, accuracy may be bad if\n               nnn is too small.  If you end the 'nnn' value with the\n               '%' character, then that percentage of the base's\n               voxels will be used.\n               [Default == 47% of voxels in the weight mask]", "help_range": [16567, 16983]}, {"param": "-nopad", "line_start": 266, "length": 3, "param_range": [16986, 16992], "help": "Do not use zero-padding on the base image.\n               [Default == zero-pad, if needed; -verb shows how much]", "help_range": [17000, 17112]}, {"param": "-zclip", "line_start": 269, "length": 5, "param_range": [17115, 17121], "help": "Replace negative values in the input datasets (source & base)\n               with zero.  The intent is to clip off a small set of negative\n               values that may arise when using 3dresample (say) with\n               cubic interpolation.", "help_range": [17129, 17373]}, {"param": "-conv", "line_start": 274, "length": 7, "param_range": [17376, 17381], "help": "Convergence test is set to 'mmm' millimeters.\n               This doesn't mean that the results will be accurate\n               to 'mmm' millimeters!  It just means that the program\n               stops trying to improve the alignment when the optimizer\n               (NEWUOA) reports it has narrowed the search radius\n               down to this level.  [Default == 0.05 mm]", "help_range": [17390, 17766]}, {"param": "-verb", "line_start": 281, "length": 2, "param_range": [17769, 17774], "help": "Print out verbose progress reports.\n               [Using '-VERB' will give even more prolix reports.]", "help_range": [17783, 17885]}, {"param": "-quiet", "line_start": 283, "length": 1, "param_range": [17887, 17893], "help": "Don't print out verbose stuff.", "help_range": [17901, 17931]}, {"param": "-usetemp", "line_start": 284, "length": 10, "param_range": [17933, 17941], "help": "Write intermediate stuff to disk, to economize on RAM.\n               Using this will slow the program down, but may make it\n               possible to register datasets that need lots of space.\n       **N.B.: Temporary files are written to the directory given\n               in environment variable TMPDIR, or in /tmp, or in ./\n               (preference in that order).  If the program crashes,\n               these files are named TIM_somethingrandom, and you\n               may have to delete them manually. (TIM=Temporary IMage)\n       **N.B.: If the program fails with a 'malloc failure' type of\n               message, then try '-usetemp' (malloc=memory allocator).", "help_range": [17947, 18619]}, {"param": "-nousetemp", "line_start": 294, "length": 2, "param_range": [18621, 18631], "help": "Don't use temporary workspace on disk [the default].", "help_range": [18635, 18687]}, {"param": "-check", "line_start": 296, "length": 10, "param_range": [18690, 18696], "help": "After cost functional optimization is done, start at the\n               final parameters and RE-optimize using the new cost\n               function 'hhh'.  If the results are too different, a\n               warning message will be printed.  However, the final\n               parameters from the original optimization will be\n               used to create the output dataset. Using '-check'\n               increases the CPU time, but can help you feel sure\n               that the alignment process did not go wild and crazy.\n               [Default == no check == don't worry, be happy!]\n       **N.B.: You can put more than one function after '-check', as in", "help_range": [18704, 19363]}, {"param": "-nmi", "line_start": 306, "length": 9, "param_range": [19381, 19385], "help": "                 -nmi -check mi hel crU crM\n               to register with Normalized Mutual Information, and\n               then check the results against 4 other cost functionals.\n       **N.B.: On the other hand, some cost functionals give better\n               results than others for specific problems, and so\n               a warning that 'mi' was significantly different than\n               'hel' might not actually mean anything useful (e.g.).\n\n ** PARAMETERS THAT AFFECT THE COST OPTIMIZATION STRATEGY **", "help_range": [19364, 19878]}, {"param": "-onepass", "line_start": 315, "length": 4, "param_range": [19880, 19888], "help": "Use only the refining pass -- do not try a coarse\n               resolution pass first.  Useful if you know that only\n               small amounts of image alignment are needed.\n               [The default is to use both passes.]", "help_range": [19894, 20123]}, {"param": "-twopass", "line_start": 319, "length": 5, "param_range": [20125, 20133], "help": "Use a two pass alignment strategy, first searching for\n               a large rotation+shift and then refining the alignment.\n               [Two passes are used by default for the first sub-brick]\n               [in the source dataset, and then one pass for the others.]\n               ['-twopass' will do two passes for ALL source sub-bricks.]", "help_range": [20139, 20484]}, {"param": "-twoblur", "line_start": 324, "length": 5, "param_range": [20486, 20494], "help": "Set the blurring radius for the first pass to 'rr'\n               millimeters.  [Default == 11 mm]\n       **N.B.: You may want to change this from the default if\n               your voxels are unusually small or unusually large\n               (e.g., outside the range 1-4 mm along each axis).", "help_range": [20500, 20792]}, {"param": "-twofirst", "line_start": 329, "length": 9, "param_range": [20794, 20803], "help": "Use -twopass on the first image to be registered, and\n               then on all subsequent images from the source dataset,\n               use results from the first image's coarse pass to start\n               the fine pass.\n               (Useful when there may be large motions between the   )\n               (source and the base, but only small motions within   )\n               (the source dataset itself; since the coarse pass can )\n               (be slow, doing it only once makes sense in this case.)\n       **N.B.: [-twofirst is on by default; '-twopass' turns it off.]", "help_range": [20808, 21386]}, {"param": "-twobest", "line_start": 338, "length": 6, "param_range": [21388, 21396], "help": "In the coarse pass, use the best 'bb' set of initial\n               points to search for the starting point for the fine\n               pass.  If bb==0, then no search is made for the best\n               starting point, and the identity transformation is\n               used as the starting point.  [Default=5; min=0 max=22]\n       **N.B.: Setting bb=0 will make things run faster, but less reliably.", "help_range": [21402, 21802]}, {"param": "-fineblur", "line_start": 344, "length": 25, "param_range": [21804, 21813], "help": "Set the blurring radius to use in the fine resolution\n               pass to 'x' mm.  A small amount (1-2 mm?) of blurring at\n               the fine step may help with convergence, if there is\n               some problem, especially if the base volume is very noisy.\n               [Default == 0 mm = no blurring at the final alignment pass]\n   **NOTES ON\n   **STRATEGY: * If you expect only small-ish (< 2 voxels?) image movement,\n                 then using '-onepass' or '-twobest 0' makes sense.\n               * If you expect large-ish image movements, then do not\n                 use '-onepass' or '-twobest 0'; the purpose of the\n                 '-twobest' parameter is to search for large initial\n                 rotations/shifts with which to start the coarse\n                 optimization round.\n               * If you have multiple sub-bricks in the source dataset,\n                 then the default '-twofirst' makes sense if you don't expect\n                 large movements WITHIN the source, but expect large motions\n                 between the source and base.\n               * '-twopass' re-starts the alignment process for each sub-brick\n                 in the source dataset -- this option can be time consuming,\n                 and is really intended to be used when you might expect large\n                 movements between sub-bricks; for example, when the different\n                 volumes are gathered on different days.  For most purposes,\n                 '-twofirst' (the default process) will be adequate and faster,\n                 when operating on multi-volume source datasets.", "help_range": [21818, 23436]}, {"param": "-cmass", "line_start": 369, "length": 5, "param_range": [23439, 23445], "help": "Use the center-of-mass calculation to bracket the shifts.\n                   [This option is OFF by default]\n                 If given in the form '-cmass+xy' (for example), means to\n                 do the CoM calculation in the x- and y-directions, but\n                 not the z-direction.", "help_range": [23455, 23747]}, {"param": "-nocmass", "line_start": 374, "length": 11, "param_range": [23749, 23757], "help": "Don't use the center-of-mass calculation. [The default]\n                  (You would not want to use the C-o-M calculation if the  )\n                  (source sub-bricks have very different spatial locations,)\n                  (since the source C-o-M is calculated from all sub-bricks)\n **EXAMPLE: You have a limited coverage set of axial EPI slices you want to\n            register into a larger head volume (after 3dSkullStrip, of course).\n            In this case, '-cmass+xy' makes sense, allowing CoM adjustment\n            along the x = R-L and y = A-P directions, but not along the\n            z = I-S direction, since the EPI doesn't cover the whole brain\n            along that axis.", "help_range": [23765, 24458]}, {"param": "-autoweight", "line_start": 385, "length": 20, "param_range": [24461, 24472], "help": "Compute a weight function using the 3dAutomask\n               algorithm plus some blurring of the base image.\n       **N.B.: '-autoweight+100' means to zero out all voxels\n                 with values below 100 before computing the weight.\n               '-autoweight**1.5' means to compute the autoweight\n                 and then raise it to the 1.5-th power (e.g., to\n                 increase the weight of high-intensity regions).\n               These two processing steps can be combined, as in\n                 '-autoweight+100**1.5'\n               ** Note that that '**' must be enclosed in quotes;\n                  otherwise, the shell will treat it as a wildcard\n                  and you will get an error message before 3dAllineate\n                  even starts!!\n       **N.B.: Some cost functionals do not allow -autoweight, and\n               will use -automask instead.  A warning message\n               will be printed if you run into this situation.\n               If a clip level '+xxx' is appended to '-autoweight',\n               then the conversion into '-automask' will NOT happen.\n               Thus, using a small positive '+xxx' can be used trick\n               -autoweight into working on any cost functional.", "help_range": [24475, 25713]}, {"param": "-automask", "line_start": 405, "length": 6, "param_range": [25715, 25724], "help": "Compute a mask function, which is like -autoweight,\n               but the weight for a voxel is set to either 0 or 1.\n       **N.B.: '-automask+3' means to compute the mask function, and\n               then dilate it outwards by 3 voxels (e.g.).\n               ** Note that '+' means something very different\n                  for '-automask' and '-autoweight'!!", "help_range": [25729, 26092]}, {"param": "-autobox", "line_start": 411, "length": 6, "param_range": [26094, 26102], "help": "Expand the -automask function to enclose a rectangular\n               box that holds the irregular mask.\n       **N.B.: This is the default mode of operation!\n               For intra-modality registration, '-autoweight' may be better!\n             * If the cost functional is 'ls', then '-autoweight' will be\n               the default, instead of '-autobox'.", "help_range": [26108, 26468]}, {"param": "-nomask", "line_start": 417, "length": 2, "param_range": [26470, 26477], "help": "Don't compute the autoweight/mask; if -weight is not\n               also used, then every voxel will be counted equally.", "help_range": [26484, 26604]}, {"param": "-weight", "line_start": 419, "length": 8, "param_range": [26606, 26613], "help": "Set the weighting for each voxel in the base dataset;\n               larger weights mean that voxel counts more in the cost\n               function.\n       **N.B.: The weight dataset must be defined on the same grid as\n               the base dataset.\n       **N.B.: Even if a method does not allow -autoweight, you CAN\n               use a weight dataset that is not 0/1 valued.  The\n               risk is yours, of course (!*! as always in AFNI !*!).", "help_range": [26620, 27073]}, {"param": "-wtprefix", "line_start": 427, "length": 3, "param_range": [27075, 27084], "help": "Write the weight volume to disk as a dataset with\n               prefix name 'p'.  Used with '-autoweight/mask', this option\n               lets you see what voxels were important in the algorithm.", "help_range": [27089, 27286]}, {"param": "-emask", "line_start": 430, "length": 18, "param_range": [27288, 27294], "help": "This option lets you specify a mask of voxels to EXCLUDE from\n               the analysis. The voxels where the dataset 'ee' is nonzero\n               will not be included (i.e., their weights will be set to zero).\n             * Like all the weight options, it applies in the base image\n               coordinate system.\n             * Like all the weight options, it means nothing if you are using\n               one of the 'apply' options.\n\n    Method  Allows -autoweight\n    ------  ------------------\n     ls     YES\n     mi     NO\n     crM    YES\n     nmi    NO\n     hel    NO\n     crA    YES\n     crU    YES", "help_range": [27302, 27916]}, {"param": "-source_mask", "line_start": 448, "length": 1, "param_range": [27919, 27931], "help": "Mask the source (input) dataset, using 'sss'.", "help_range": [27938, 27983]}, {"param": "-source_automask", "line_start": 449, "length": 6, "param_range": [27985, 28001], "help": "Automatically mask the source dataset.\n                      [By default, all voxels in the source]\n                      [dataset are used in the matching.   ]\n            **N.B.: You can also use '-source_automask+3' to dilate\n                    the default source automask outward by 3 voxels.", "help_range": [28004, 28301]}, {"param": "-warp", "line_start": 455, "length": 8, "param_range": [28304, 28309], "help": "Set the warp type to 'xxx', which is one of\n                 shift_only         *OR* sho =  3 parameters\n                 shift_rotate       *OR* shr =  6 parameters\n                 shift_rotate_scale *OR* srs =  9 parameters\n                 affine_general     *OR* aff = 12 parameters\n               [Default = affine_general, which includes image]\n               [      shifts, rotations, scaling, and shearing]", "help_range": [28318, 28733]}, {"param": "-warpfreeze", "line_start": 463, "length": 5, "param_range": [28736, 28747], "help": "Freeze the non-rigid body parameters (those past #6)\n               after doing the first sub-brick.  Subsequent volumes\n               will have the same spatial distortions as sub-brick #0,\n               plus rigid body motions only.", "help_range": [28750, 28986]}, {"param": "-replacebase", "line_start": 468, "length": 5, "param_range": [28989, 29001], "help": "If the source has more than one sub-brick, and this\n                  option is turned on, then after the #0 sub-brick is\n                  aligned to the base, the aligned #0 sub-brick is used\n                  as the base image for subsequent source sub-bricks.", "help_range": [29006, 29269]}, {"param": "-replacemeth", "line_start": 473, "length": 3, "param_range": [29272, 29284], "help": "After sub-brick #0 is aligned, switch to method 'm'\n                  for later sub-bricks.  For use with '-replacebase'.", "help_range": [29289, 29410]}, {"param": "-EPI", "line_start": 476, "length": 18, "param_range": [29413, 29417], "help": "Treat the source dataset as being composed of warped\n               EPI slices, and the base as comprising anatomically\n               'true' images.  Only phase-encoding direction image\n               shearing and scaling will be allowed with this option.\n       **N.B.: For most people, the base dataset will be a 3dSkullStrip-ed\n               T1-weighted anatomy (MPRAGE or SPGR).  If you don't remove\n               the skull first, the EPI images (which have little skull\n               visible due to fat-suppression) might expand to fit EPI\n               brain over T1-weighted skull.\n       **N.B.: Usually, EPI datasets don't have as complete slice coverage\n               of the brain as do T1-weighted datasets.  If you don't use\n               some option (like '-EPI') to suppress scaling in the slice-\n               direction, the EPI dataset is likely to stretch the slice\n               thicknesss to better 'match' the T1-weighted brain coverage.\n       **N.B.: '-EPI' turns on '-warpfreeze -replacebase'.\n               You can use '-nowarpfreeze' and/or '-noreplacebase' AFTER the\n               '-EPI' on the command line if you do not want these options used.", "help_range": [29427, 30610]}, {"param": "-parfix", "line_start": 494, "length": 1, "param_range": [30613, 30620], "help": "Fix parameter #n to be exactly at value 'v'.", "help_range": [30629, 30673]}, {"param": "-parang", "line_start": 495, "length": 2, "param_range": [30675, 30682], "help": "Allow parameter #n to range only between 'b' and 't'.\n                 If not given, default ranges are used.", "help_range": [30691, 30800]}, {"param": "-parini", "line_start": 497, "length": 8, "param_range": [30802, 30809], "help": "Initialize parameter #n to value 'v', but then\n                 allow the algorithm to adjust it.\n         **N.B.: Multiple '-par...' options can be used, to constrain\n                 multiple parameters.\n         **N.B.: -parini has no effect if -twopass is used, since\n                 the -twopass algorithm carries out its own search\n                 for initial parameters.", "help_range": [30818, 31197]}, {"param": "-maxrot", "line_start": 505, "length": 3, "param_range": [31200, 31207], "help": "Allow maximum rotation of 'dd' degrees.  Equivalent\n                 to '-parang 4 -dd dd -parang 5 -dd dd -parang 6 -dd dd'\n                 [Default=30 degrees]", "help_range": [31216, 31378]}, {"param": "-maxshf", "line_start": 508, "length": 5, "param_range": [31380, 31387], "help": "Allow maximum shift of 'dd' millimeters.  Equivalent\n                 to '-parang 1 -dd dd -parang 2 -dd dd -parang 3 -dd dd'\n                 [Default=32% of the size of the base image]\n         **N.B.: This max shift setting is relative to the center-of-mass\n                 shift, if the '-cmass' option is used.", "help_range": [31396, 31712]}, {"param": "-maxscl", "line_start": 513, "length": 3, "param_range": [31714, 31721], "help": "Allow maximum scaling factor to be 'dd'.  Equivalent\n                 to '-parang 7 1/dd dd -parang 8 1/dd dd -paran2 9 1/dd dd'\n                 [Default=1.2=image can go up or down 20% in size]", "help_range": [31730, 31925]}, {"param": "-maxshr", "line_start": 516, "length": 8, "param_range": [31927, 31934], "help": "Allow maximum shearing factor to be 'dd'. Equivalent\n                 to '-parang 10 -dd dd -parang 11 -dd dd -parang 12 -dd dd'\n                 [Default=0.1111 for no good reason]\n\n NOTE: If the datasets being registered have only 1 slice, 3dAllineate\n       will automatically fix the 6 out-of-plane motion parameters to\n       their 'do nothing' values, so you don't have to specify '-parfix'.", "help_range": [31943, 32340]}, {"param": "-master", "line_start": 524, "length": 12, "param_range": [32343, 32350], "help": "Write the output dataset on the same grid as dataset\n               'mmm'.  If this option is NOT given, the base dataset\n               is the master.\n       **N.B.: 3dAllineate transforms the source dataset to be 'similar'\n               to the base image.  Therefore, the coordinate system\n               of the master dataset is interpreted as being in the\n               reference system of the base image.  It is thus vital\n               that these finite 3D volumes overlap, or you will lose data!\n       **N.B.: If 'mmm' is the string 'SOURCE', then the source dataset\n               is used as the master for the output dataset grid.\n               You can also use 'BASE', which is of course the default.", "help_range": [32357, 33072]}, {"param": "-mast_dxyz", "line_start": 536, "length": 2, "param_range": [33075, 33085], "help": "Write the output dataset using grid spacings of\n  *OR*            'del' mm.  If this option is NOT given, then the", "help_range": [33092, 33206]}, {"param": "-newgrid", "line_start": 538, "length": 68, "param_range": [33208, 33216], "help": " -newgrid del     grid spacings in the master dataset will be used.\n                  This option is useful when registering low resolution\n                  data (e.g., EPI time series) to high resolution\n                  datasets (e.g., MPRAGE) where you don't want to\n                  consume vast amounts of disk space interpolating\n                  the low resolution data to some artificially fine\n                  (and meaningless) spatial grid.\n\n----------------------------------------------\nDEFINITION OF AFFINE TRANSFORMATION PARAMETERS\n----------------------------------------------\nThe 3x3 spatial transformation matrix is calculated as [S][D][U],\nwhere [S] is the shear matrix,\n      [D] is the scaling matrix, and\n      [U] is the rotation (proper orthogonal) matrix.\nThes matrices are specified in DICOM-ordered (x=-R+L,y=-A+P,z=-I+S)\ncoordinates as:\n\n  [U] = [Rotate_y(param#6)] [Rotate_x(param#5)] [Rotate_z(param #4)]\n        (angles are in degrees)\n\n  [D] = diag( param#7 , param#8 , param#9 )\n\n        [    1        0     0 ]        [ 1 param#10 param#11 ]\n  [S] = [ param#10    1     0 ]   OR   [ 0    1     param#12 ]\n        [ param#11 param#12 1 ]        [ 0    0        1     ]\n\nThe shift vector comprises parameters #1, #2, and #3.\n\nThe goal of the program is to find the warp parameters such that\n   I([x]_warped) 'is similar to' J([x]_in)\nas closely as possible in some sense of 'similar', where J(x) is the\nbase image, and I(x) is the source image.\n\nUsing '-parfix', you can specify that some of these parameters\nare fixed.  For example, '-shift_rotate_scale' is equivalent\n'-affine_general -parfix 10 0 -parfix 11 0 -parfix 12 0'.\nDon't even think of using the '-parfix' option unless you grok\nthis example!\n\n----------- Special Note for the '-EPI' Option's Coordinates -----------\nIn this case, the parameters above are with reference to coordinates\n  x = frequency encoding direction (by default, first axis of dataset)\n  y = phase encoding direction     (by default, second axis of dataset)\n  z = slice encoding direction     (by default, third axis of dataset)\nThis option lets you freeze some of the warping parameters in ways that\nmake physical sense, considering how echo-planar images are acquired.\nThe x- and z-scaling parameters are disabled, and shears will only affect\nthe y-axis.  Thus, there will be only 9 free parameters when '-EPI' is\nused.  If desired, you can use a '-parang' option to allow the scaling\nfixed parameters to vary (put these after the '-EPI' option):\n  -parang 7 0.833 1.20     to allow x-scaling\n  -parang 9 0.833 1.20     to allow z-scaling\nYou could also fix some of the other parameters, if that makes sense\nin your situation; for example, to disable out-of-slice rotations:\n  -parfix 5 0  -parfix 6 0\nand to disable out of slice translation:\n  -parfix 3 0\nNOTE WELL: If you use '-EPI', then the output warp parameters (e.g., in\n           '-1Dparam_save') apply to the (freq,phase,slice) xyz coordinates,\n           NOT to the DICOM xyz coordinates, so equivalent transformations\n           will be expressed with different sets of parameters entirely\n           than if you don't use '-EPI'!  This comment does NOT apply\n           to the output of '-1Dmatrix_save', since that matrix is\n           defined relative to the RAI (DICOM) spatial coordinates.\n\n*********** CHANGING THE ORDER OF MATRIX APPLICATION ***********", "help_range": [33207, 36603]}, {"param": "-SDU", "line_start": 606, "length": 1, "param_range": [36607, 36611], "help": "  -SDU or -SUD }= Set the order of the matrix multiplication", "help_range": [36605, 36665]}, {"param": "-DSU", "line_start": 607, "length": 1, "param_range": [36668, 36672], "help": "  -DSU or -DUS }= for the affine transformations:", "help_range": [36666, 36715]}, {"param": "-USD", "line_start": 608, "length": 7, "param_range": [36718, 36722], "help": "  -USD or -UDS }=   S = triangular shear (params #10-12)\n                    D = diagonal scaling matrix (params #7-9)\n                    U = rotation matrix (params #4-6)\n                  Default order is '-SDU', which means that\n                  the U matrix is applied first, then the\n                  D matrix, then the S matrix.", "help_range": [36716, 37053]}, {"param": "-Supper", "line_start": 615, "length": 1, "param_range": [37057, 37064], "help": " Set the S matrix to be upper or lower", "help_range": [37072, 37110]}, {"param": "-Slower", "line_start": 616, "length": 2, "param_range": [37113, 37120], "help": " triangular [Default=lower triangular]", "help_range": [37128, 37166]}, {"param": "-ashift", "line_start": 618, "length": 1, "param_range": [37170, 37177], "help": " Apply the shift parameters (#1-3) after OR", "help_range": [37185, 37228]}, {"param": "-bshift", "line_start": 619, "length": 16, "param_range": [37231, 37238], "help": " before the matrix transformation. [Default=after]\n\n            ==================================================\n        ===== RWCox - September 2006 - Live Long and Prosper =====\n            ==================================================\n\n         ********************************************************\n        *** From Webster's Dictionary: Allineate == 'to align' ***\n         ********************************************************\n\n===========================================================================\n                       FORMERLY SECRET HIDDEN OPTIONS\n---------------------------------------------------------------------------\n        ** N.B.: Most of these are experimental! [permanent beta] **\n===========================================================================", "help_range": [37246, 38042]}, {"param": "-num_rtb", "line_start": 635, "length": 17, "param_range": [38045, 38053], "help": "At the beginning of the fine pass, the best set of results\n               from the coarse pass are 'refined' a little by further\n               optimization, before the single best one is chosen for\n               for the final fine optimization.\n              * This option sets the maximum number of cost functional\n                evaluations to be used (for each set of parameters)\n                in this step.\n              * The default is 99; a larger value will take more CPU\n                time but may give more robust results.\n              * If you want to skip this step entirely, use '-num_rtb 0'.\n                then, the best of the coarse pass results is taken\n                straight to the final optimization passes.\n       **N.B.: If you use '-VERB', you will see that one extra case\n               is involved in this initial fine refinement step; that\n               case is starting with the identity transformation, which\n               helps insure against the chance that the coarse pass\n               optimizations ran totally amok.", "help_range": [38059, 39123]}, {"param": "-nocast", "line_start": 652, "length": 4, "param_range": [39125, 39132], "help": "By default, parameter vectors that are too close to the\n               best one are cast out at the end of the coarse pass\n               refinement process. Use this option if you want to keep\n               them all for the fine resolution pass.", "help_range": [39139, 39386]}, {"param": "-norefinal", "line_start": 656, "length": 7, "param_range": [39388, 39398], "help": "Do NOT re-start the fine iteration step after it\n               has converged.  The default is to re-start it, which\n               usually results in a small improvement to the result\n               (at the cost of CPU time).  This re-start step is an\n               an attempt to avoid a local minimum trap.  It is usually\n               not necessary, but sometimes helps.", "help_range": [39402, 39777]}, {"param": "-realaxes", "line_start": 663, "length": 3, "param_range": [39780, 39789], "help": "Use the 'real' axes stored in the dataset headers, if they\n               conflict with the default axes.  [For Jedi AFNI Masters only!]", "help_range": [39794, 39930]}, {"param": "-savehist", "line_start": 666, "length": 8, "param_range": [39933, 39942], "help": "Save start and final 2D histograms as PGM\n                 files, with prefix 'sss' (cost: cr mi nmi hel).\n                * if filename contains 'FF', floats is written\n                * these are the weighted histograms!\n                * -savehist will also save histogram files when\n                  the -allcost evaluations takes place\n                * this option is mostly useless unless '-histbin' is\n                  also used", "help_range": [39949, 40387]}, {"param": "-median", "line_start": 674, "length": 2, "param_range": [40389, 40396], "help": "Smooth with median filter instead of Gaussian blur.\n                 (Somewhat slower, and not obviously useful.)", "help_range": [40405, 40518]}, {"param": "-powell", "line_start": 676, "length": 8, "param_range": [40520, 40527], "help": "Set the Powell NEWUOA dimensional parameters to\n                 'm' and 'a' (cf. source code in powell_int.c).\n                 The number of points used for approximating the\n                 cost functional is m*N+a, where N is the number\n                 of parameters being optimized.  The default values\n                 are m=2 and a=3.  Larger values will probably slow\n                 the program down for no good reason.  The smallest\n                 allowed values are 1.", "help_range": [40536, 41020]}, {"param": "-target", "line_start": 684, "length": 7, "param_range": [41022, 41029], "help": "Same as '-source ttt'.  In the earliest versions,\n                 what I now call the 'source' dataset was called the\n                 'target' dataset:\n                    Try to remember the kind of September (2006)\n                    When life was slow and oh so mellow\n                    Try to remember the kind of September\n                    When grass was green and source was target.", "help_range": [41038, 41434]}, {"param": "-Xwarp", "line_start": 691, "length": 1, "param_range": [41436, 41442], "help": " Change the warp/matrix setup so that only the x-, y-, or z-", "help_range": [41451, 41511]}, {"param": "-Ywarp", "line_start": 692, "length": 1, "param_range": [41513, 41519], "help": " axis is stretched & sheared.  Useful for EPI, where 'X',", "help_range": [41528, 41585]}, {"param": "-Zwarp", "line_start": 693, "length": 1, "param_range": [41587, 41593], "help": " 'Y', or 'Z' corresponds to the phase encoding direction.", "help_range": [41602, 41659]}, {"param": "-FPS", "line_start": 694, "length": 1, "param_range": [41661, 41665], "help": "Generalizes -EPI to arbitrary permutation of directions.", "help_range": [41677, 41733]}, {"param": "-histpow", "line_start": 695, "length": 5, "param_range": [41735, 41743], "help": "By default, the number of bins in the histogram used\n                 for calculating the Hellinger, Mutual Information, and\n                 Correlation Ratio statistics is n^(1/3), where n is\n                 the number of data points.  You can change that exponent\n                 to 'pp' with this option.", "help_range": [41751, 42061]}, {"param": "-histbin", "line_start": 700, "length": 1, "param_range": [42063, 42071], "help": "Or you can just set the number of bins directly to 'nn'.", "help_range": [42079, 42135]}, {"param": "-eqbin", "line_start": 701, "length": 1, "param_range": [42137, 42143], "help": "Use equalized marginal histograms with 'nn' bins.", "help_range": [42153, 42202]}, {"param": "-clbin", "line_start": 702, "length": 10, "param_range": [42204, 42210], "help": "Use 'nn' equal-spaced bins except for the bot and top,\n                 which will be clipped (thus the 'cl').  If nn is 0, the\n                 program will pick the number of bins for you.\n                 **N.B.: '-clbin 0' is now the default [25 Jul 2007];\n                         if you want the old all-equal-spaced bins, use\n                         '-histbin 0'.\n                 **N.B.: '-clbin' only works when the datasets are\n                         non-negative; any negative voxels in either\n                         the input or source volumes will force a switch\n                         to all equal-spaced bins.", "help_range": [42220, 42851]}, {"param": "-wtmrad", "line_start": 712, "length": 1, "param_range": [42853, 42860], "help": "Set autoweight/mask median filter radius to 'mm' voxels.", "help_range": [42869, 42925]}, {"param": "-wtgrad", "line_start": 713, "length": 1, "param_range": [42927, 42934], "help": "Set autoweight/mask Gaussian filter radius to 'gg' voxels.", "help_range": [42943, 43001]}, {"param": "-nmsetup", "line_start": 714, "length": 1, "param_range": [43003, 43011], "help": "Use 'nn' points for the setup matching [default=98756]", "help_range": [43019, 43073]}, {"param": "-ignout", "line_start": 715, "length": 2, "param_range": [43075, 43082], "help": "Ignore voxels outside the warped source dataset.", "help_range": [43091, 43139]}, {"param": "-blok", "line_start": 717, "length": 9, "param_range": [43142, 43147], "help": "Blok definition for the 'lp?' (Local Pearson) cost\n                 functions: 'bbb' is one of\n                   'BALL(r)' or 'CUBE(r)' or 'RHDD(r)' or 'TOHD(r)'\n                 corresponding to\n                   spheres or cubes or rhombic dodecahedra or\n                   truncated octahedra\n                 where 'r' is the size parameter in mm.\n                 [Default is 'RHDD(6.54321)' (rhombic dodecahedron)]", "help_range": [43158, 43580]}, {"param": "-allcost", "line_start": 726, "length": 2, "param_range": [43583, 43591], "help": "Compute ALL available cost functionals and print them\n                   at various points.", "help_range": [43601, 43692]}, {"param": "-allcostX", "line_start": 728, "length": 2, "param_range": [43694, 43703], "help": "Compute and print ALL available cost functionals for the\n                   un-warped inputs, and then quit.", "help_range": [43712, 43820]}, {"param": "-allcostX1D", "line_start": 730, "length": 19, "param_range": [43822, 43833], "help": "Compute ALL available cost functionals for the set of\n                   parameters given in the 1D file 'p' (12 values per row),\n                   write them to the 1D file 'q', then exit. (For you, Zman)\n                  * N.B.: If -fineblur is used, that amount of smoothing\n                          will be applied prior to the -allcostX evaluations.\n                          The parameters are the rotation, shift, scale,\n                          and shear values, not the affine transformation\n                          matrix. An identity matrix could be provided as\n                          \"0 0 0  0 0 0  1 1 1  0 0 0\" for instance or by\n                          using the word \"IDENTITY\"\n\n===========================================================================\n\nModifying '-final wsinc5'\n-------------------------\n * The windowed (tapered) sinc function interpolation can be modified\n     by several environment variables.  This is expert-level stuff, and\n     you should understand what you are doing if you use these options.\n     The simplest way to use these would be on the command line, as in", "help_range": [43840, 44959]}, {"param": "-DAFNI_WSINC5_RADIUS", "line_start": 749, "length": 81, "param_range": [44967, 44987], "help": " -DAFNI_WSINC5_TAPERFUN=Hamming\n\n * AFNI_WSINC5_TAPERFUN lets you choose the taper function.\n     The default taper function is the minimum sidelobe 3-term cosine:\n       0.4243801 + 0.4973406*cos(PI*x) + 0.0782793*cos(2*PI*x)\n     If you set this environment variable to 'Hamming', then the\n     minimum sidelobe 2-term cosine will be used instead:\n       0.53836 + 0.46164*cos(PI*x)\n     Here, 'x' is between 0 and 1, where x=0 is the center of the\n     interpolation mask and x=1 is the outer edge.\n ++  Unfortunately, the 3-term cosine doesn't have a catchy name; you can\n       find it (and many other) taper functions described in the paper\n         AH Nuttall, Some Windows with Very Good Sidelobe Behavior.\n         IEEE Trans. ASSP, 29:84-91 (1981).\n       In particular, see Fig.14 and Eq.36 in this paper.\n\n * AFNI_WSINC5_TAPERCUT lets you choose the start 'x' point for tapering:\n     This value should be between 0 and 0.8; for example, 0 means to taper\n     all the way from x=0 to x=1 (maximum tapering).  The default value\n     is 0.  Setting TAPERCUT to 0.5 (say) means only to taper from x=0.5\n     to x=1; thus, a larger value means that fewer points are tapered\n     inside the interpolation mask.\n\n * AFNI_WSINC5_RADIUS lets you choose the radius of the tapering window\n     (i.e., the interpolation mask region).  This value is an integer\n     between 3 and 21.  The default value is 5 (which used to be the\n     ONLY value, thus 'wsinc5').  RADIUS is measured in voxels, not mm.\n\n * AFNI_WSINC5_SPHERICAL lets you choose the shape of the mask region.\n     If you set this value to 'Yes', then the interpolation mask will be\n     spherical; otherwise, it defaults to cubical.\n\n * The Hamming taper function is a little faster than the 3-term function,\n     but will have a little more Gibbs phenomenon.\n * A larger TAPERCUT will give a little more Gibbs phenomenon; compute\n     speed won't change much with this parameter.\n * Compute time goes up with (at least) the 3rd power of the RADIUS; setting\n     RADIUS to 21 will be VERY slow.\n * Visually, RADIUS=3 is similar to quintic interpolation.  Increasing\n     RADIUS makes the interpolated images look sharper and more well-\n     defined.  However, values of RADIUS greater than or equal to 7 appear\n     (to Zhark's eagle eye) to be almost identical.  If you really care,\n     you'll have to experiment with this parameter yourself.\n * A spherical mask is also VERY slow, since the cubical mask allows\n     evaluation as a tensor product.  There is really no good reason\n     to use a spherical mask; I only put it in for experimental purposes.\n** For most users, there is NO reason to ever use these environment variables\n     to modify wsinc5.  You should only do this kind of thing if you have a\n     good and articulable reason!  (Or if you really like to screw around.)\n** The wsinc5 interpolation function is parallelized using OpenMP, which\n     makes its usage moderately tolerable.\n\n===========================================================================\n\nHidden experimental cost functionals:\n-------------------------------------\n   sp   *OR*  spearman        = Spearman [rank] Correlation\n   je   *OR*  jointentropy    = Joint Entropy [H(b,s)]\n   lss  *OR*  signedPcor      = Signed Pearson Correlation\n   lpc  *OR*  localPcorSigned = Local Pearson Correlation Signed\n   lpa  *OR*  localPcorAbs    = Local Pearson Correlation Abs\n   lpc+ *OR*  localPcor+Others= Local Pearson Signed + Others\n   ncd  *OR*  NormCompDist    = Normalized Compression Distance\n\nNotes for the new [Feb 2010] lpc+ cost functional:\n--------------------------------------------------\n * The cost functional named 'lpc+' is a combination of several others:\n     lpc + hel*0.4 + crA*0.4 + nmi*0.2 + mi*0.2 + ov*0.4\n   ++ 'hel', 'crA', 'nmi', and 'mi' are the histogram-based cost\n      functionals also available as standalone options.\n   ++ 'ov' is a measure of the overlap of the automasks of the base and\n      source volumes; ov is not available as a standalone option.\n * The purpose of lpc+ is to avoid situations where the pure lpc cost\n   goes wild; this especially happens if '-source_automask' isn't used.\n   ++ Even with lpc+, you should use '-source_automask+2' (say) to be safe.\n * You can alter the weighting of the extra functionals by giving the\n   option in the form (for example)\n     '-lpc+hel*0.5+nmi*0+mi*0+crA*1.0+ov*0.5'\n * The quotes are needed to prevent the shell from wild-card expanding\n   the '*' character.\n   --> You can now use ':' in place of '*' to avoid this wildcard problem:", "help_range": [44989, 49566]}, {"param": "-lpc+hel:0.5+nmi:0+mi:0+crA:1+ov:0.5+ZZ", "line_start": 830, "length": 82, "param_range": [49576, 49615], "help": "         -lpc+hel:0.5+nmi:0+mi:0+crA:1+ov:0.5+ZZ\n * Notice the weight factors FOLLOW the name of the extra functionals.\n   ++ If you want a weight to be 0 or 1, you have to provide for that\n      explicitly -- if you leave a weight off, then it will get its\n      default value!\n   ++ The order of the weight factor names is unimportant here:\n        '-lpc+hel*0.5+nmi*0.8' == '-lpc+nmi*0.8+hel*0.5'\n * Only the 5 functionals listed (hel,crA,nmi,mi,ov) can be used in '-lpc+'.\n * In addition, if you want the initial alignments to be with '-lpc+' and\n   then finish the Final alignment with pure '-lpc', you can indicate this\n   by putting 'ZZ' somewhere in the option string, as in '-lpc+ZZ'.\n * This stuff should be considered really experimental at this moment!\n\nCost functional descriptions (for use with -allcost output):\n------------------------------------------------------------\n   ls  :: 1 - abs(Pearson correlation coefficient)\n   sp  :: 1 - abs(Spearman correlation coefficient)\n   mi  :: - Mutual Information = H(base,source)-H(base)-H(source)\n   crM :: 1 - abs[ CR(base,source) * CR(source,base) ]\n   nmi :: 1/Normalized MI = H(base,source)/[H(base)+H(source)]\n   je  :: H(base,source) = joint entropy of image pair\n   hel :: - Hellinger distance(base,source)\n   crA :: 1 - abs[ CR(base,source) + CR(source,base) ]\n   crU :: CR(source,base) = Var(source|base) / Var(source)\n   lss :: Pearson correlation coefficient between image pair\n   lpc :: nonlinear average of Pearson cc over local neighborhoods\n   lpa :: 1 - abs(lpc)\n   lpc+:: lpc + hel + mi + nmi + crA + overlap\n   ncd :: mutual compressibility (via zlib) -- doesn't work yet\n\n * N.B.: Some cost functional values (as printed out above)\n   are negated from their theoretical descriptions (e.g., 'hel')\n   so that the best image alignment will be found when the cost\n   is minimized.  See the descriptions above and the references\n   below for more details for each functional.\n\n * For more information about the 'lpc' functional, see\n     ZS Saad, DR Glen, G Chen, MS Beauchamp, R Desai, RW Cox.\n       A new method for improving functional-to-structural\n       MRI alignment using local Pearson correlation.\n       NeuroImage 44: 839-848, 2009.\n     http://dx.doi.org/10.1016/j.neuroimage.2008.09.037\n     https://afni.nimh.nih.gov/sscc/rwcox/papers/LocalPearson2009.pdf\n   The '-blok' option can be used to control the regions\n   (size and shape) used to compute the local correlations.\n *** Using the 'lpc' functional wisely requires the use of\n     a proper weight volume.  We HIGHLY recommend you use\n     the align_epi_anat.py script if you want to use this\n     cost functional!  Otherwise, you are likely to get\n     less than optimal results (and then swear at us unjustly).\n\n * For more information about the 'cr' functionals, see\n     http://en.wikipedia.org/wiki/Correlation_ratio\n   Note that CR(x,y) is not the same as CR(y,x), which\n   is why there are symmetrized versions of it available.\n\n * For more information about the 'mi', 'nmi', and 'je'\n   cost functionals, see\n     http://en.wikipedia.org/wiki/Mutual_information\n     http://en.wikipedia.org/wiki/Joint_entropy\n     http://www.cs.jhu.edu/~cis/cista/746/papers/mutual_info_survey.pdf\n\n * For more information about the 'hel' functional, see\n     http://en.wikipedia.org/wiki/Hellinger_distance\n\n * Some cost functionals (e.g., 'mi', 'cr', 'hel') are\n   computed by creating a 2D joint histogram of the\n   base and source image pair.  Various options above\n   (e.g., '-histbin', etc.) can be used to control the\n   number of bins used in the histogram on each axis.\n   (If you care to control the program in such detail!)\n\n * Minimization of the chosen cost functional is done via\n   the NEWUOA software, described in detail in\n     MJD Powell. 'The NEWUOA software for unconstrained\n       optimization without derivatives.' In: GD Pillo,\n       M Roma (Eds), Large-Scale Nonlinear Optimization.\n       Springer, 2006.\n     http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2004_08.pdf\n\n===========================================================================", "help_range": [49567, 53677]}, {"param": "-nwarp", "line_start": 912, "length": null, "param_range": [53680, 53686], "help": "Experimental nonlinear warping:\n\n              ***** Note that these '-nwarp' options are superseded  *****\n              ***** by the AFNI program 3dQwarp,  which does a more  *****\n              ***** accurate and better and job of nonlinear warping *****\n              ***** ------ Zhark the Warper ------ July 2013 ------- *****\n\n              * At present, the only 'type' is 'bilinear',\n                as in 3dWarpDrive, with 39 parameters.\n              * I plan to implement more complicated nonlinear\n                warps in the future, someday ....\n              * -nwarp can only be applied to a source dataset\n                that has a single sub-brick!\n              * -1Dparam_save and -1Dparam_apply work with\n                bilinear warps; see the Notes for more information.\n        ==>>*** Nov 2010: I have now added the following polynomial\n                warps: 'cubic', 'quintic', 'heptic', 'nonic' (using\n                3rd, 5th, 7th, and 9th order Legendre polynomials); e.g.,\n                   -nwarp heptic\n              * These are the nonlinear warps that I now am supporting.\n              * Or you can call them 'poly3', 'poly5', 'poly7', and 'poly9',\n                  for simplicity and non-Hellenistic clarity.\n              * These names are not case sensitive: 'nonic' == 'Nonic', etc.\n              * Higher and higher order polynomials will take longer and longer\n                to run!\n              * If you wish to apply a nonlinear warp, you have to supply\n                a parameter file with -1Dparam_apply and also specify the\n                warp type with -nwarp.  The number of parameters in the\n                file (per line) must match the warp type:\n                   bilinear =  43   [for all nonlinear warps, the final]\n                   cubic    =  64   [4 'parameters' are fixed values to]\n                   quintic  = 172   [normalize the coordinates to -1..1]\n                   heptic   = 364   [for the nonlinear warp functions. ]\n                   nonic    = 664\n                In all these cases, the first 12 parameters are the\n                affine parameters (shifts, rotations, etc.), and the\n                remaining parameters define the nonlinear part of the warp\n                (polynomial coefficients); thus, the number of nonlinear\n                parameters over which the optimization takes place is\n                the number in the table above minus 16.\n               * The actual polynomial functions used are products of\n                 Legendre polynomials, but the symbolic names used in\n                 the header line in the '-1Dparam_save' output just\n                 express the polynomial degree involved; for example,\n                      quint:x^2*z^3:z\n                 is the name given to the polynomial warp basis function\n                 whose highest power of x is 2, is independent of y, and\n                 whose highest power of z is 3; the 'quint' indicates that\n                 this was used in '-nwarp quintic'; the final ':z' signifies\n                 that this function was for deformations in the (DICOM)\n                 z-direction (+z == Superior).\n        ==>>*** You can further control the form of the polynomial warps\n                (but not the bilinear warp!) by restricting their degrees\n                of freedom in 2 different ways.\n                ++ You can remove the freedom to have the nonlinear\n                   deformation move along the DICOM x, y, and/or z axes.\n                ++ You can remove the dependence of the nonlinear\n                   deformation on the DICOM x, y, and/or z coordinates.\n                ++ To illustrate with the six second order polynomials:\n                      p2_xx(x,y,z) = x*x  p2_xy(x,y,z) = x*y\n                      p2_xz(x,y,z) = x*z  p2_yy(x,y,z) = y*y\n                      p2_yz(x,y,z) = y*z  p2_zz(x,y,z) = z*z\n                   Unrestricted, there are 18 parameters associated with\n                   these polynomials, one for each direction of motion (x,y,z)\n                   * If you remove the freedom of the nonlinear warp to move\n                     data in the z-direction (say), then there would be 12\n                     parameters left.\n                   * If you instead remove the freedom of the nonlinear warp\n                     to depend on the z-coordinate, you would be left with\n                     3 basis functions (p2_xz, p2_yz, and p2_zz would be\n                     eliminated), each of which would have x-motion, y-motion,\n                     and z-motion parameters, so there would be 9 parameters.\n                ++ To fix motion along the x-direction, use the option\n                   '-nwarp_fixmotX' (and '-nwarp_fixmotY' and '-nwarp_fixmotZ).\n                ++ To fix dependence of the polynomial warp on the x-coordinate,\n                   use the option '-nwarp_fixdepX' (et cetera).\n                ++ These coordinate labels in the options (X Y Z) refer to the\n                   DICOM directions (X=R-L, Y=A-P, Z=I-S).  If you would rather\n                   fix things along the dataset storage axes, you can use\n                   the symbols I J K to indicate the fastest to slowest varying\n                   array dimensions (e.g., '-nwarp_fixdepK').\n                   * Mixing up the X Y Z and I J K forms of parameter freezing\n                     (e.g., '-nwarp_fixmotX -nwarp_fixmotJ') may cause trouble!\n                ++ If you input a 2D dataset (a single slice) to be registered\n                   with '-nwarp', the program automatically assumes '-nwarp_fixmotK'\n                   and '-nwarp_fixdepK' so there are no out-of-plane parameters\n                   or dependence.  The number of nonlinear parameters is then:\n                     2D: cubic = 14 ; quintic =  36 ; heptic =  66 ; nonic = 104.\n                     3D: cubic = 48 ; quintic = 156 ; heptic = 348 ; nonic = 648.\n                     [ n-th order: 2D = (n+4)*(n-1) ; 3D = (n*n+7*n+18)*(n-1)/2 ]\n                ++ Note that these '-nwarp_fix' options have no effect on the\n                   affine part of the warp -- if you want to constrain that as\n                   well, you'll have to use the '-parfix' option.\n                   * However, for 2D images, the affine part will automatically\n                     be restricted to in-plane (6 parameter) 'motions'.\n                ++ If you save the warp parameters (with '-1Dparam_save') when\n                   doing 2D registration, all the parameters will be saved, even\n                   the large number of them that are fixed to zero. You can use\n                   '1dcat -nonfixed' to remove these columns from the 1D file if\n                   you want to further process the varying parameters (e.g., 1dsvd).\n              **++ The mapping from I J K to X Y Z (DICOM coordinates), where the\n                   '-nwarp_fix' constraints are actually applied, is very simple:\n                   given the command to fix K (say), the coordinate X, or Y, or Z\n                   whose direction most closely aligns with the dataset K grid\n                   direction is chosen.  Thus, for coronal images, K is in the A-P\n                   direction, so '-nwarp_fixmotK' is translated to '-nwarp_fixmotY'.\n                   * This simplicity means that using the '-nwarp_fix' commands on\n                     oblique datasets is problematic.  Perhaps it would work in\n                     combination with the '-EPI' option, but that has not been tested.\n\n-nwarp NOTES:\n-------------\n* -nwarp is slow - reeeaaallll slow - use it with OpenMP!\n* Check the results to make sure the optimizer didn't run amok!\n   (You should ALWAYS do this with any registration software.)\n* For the nonlinear warps, the largest coefficient allowed is\n   set to 0.10 by default.  If you wish to change this, use an\n   option like '-nwarp_parmax 0.05' (to make the allowable amount\n   of nonlinear deformation half the default).\n  ++ N.B.: Increasing the maximum past 0.10 may give very bad results!!\n* If you use -1Dparam_save, then you can apply the nonlinear\n   warp to another dataset using -1Dparam_apply in a later\n   3dAllineate run. To do so, use '-nwarp xxx' in both runs\n   , so that the program knows what the extra parameters in\n   the file are to be used for.\n  ++ Bilinear: 43 values are saved in 1 row of the param file.\n  ++ The first 12 are the affine parameters\n  ++ The next 27 are the D1,D2,D3 matrix parameters (cf. infra).\n  ++ The final 'extra' 4 values are used to specify\n      the center of coordinates (vector Xc below), and a\n      pre-computed scaling factor applied to parameters #13..39.\n  ++ For polynomial warps, a similar format is used (mutatis mutandis).\n* The option '-nwarp_save sss' lets you save a 3D dataset of the\n  the displacement field used to create the output dataset.  This\n  dataset can be used in program 3dNwarpApply to warp other datasets.\n  ++ If the warp is symbolized by x -> w(x) [here, x is a DICOM 3-vector],\n     then the '-nwarp_save' dataset contains w(x)-x; that is, it contains\n     the warp displacement of each grid point from its grid location.\n  ++ Also see program 3dNwarpCalc for other things you can do with this file:\n       warp inversion, catenation, square root, ...\n\n* Bilinear warp formula:\n   Xout = inv[ I + {D1 (Xin-Xc) | D2 (Xin-Xc) | D3 (Xin-Xc)} ] [ A Xin ]\n  where Xin  = input vector  (base dataset coordinates)\n        Xout = output vector (source dataset coordinates)\n        Xc   = center of coordinates used for nonlinearity\n               (will be the center of the base dataset volume)\n        A    = matrix representing affine transformation (12 params)\n        I    = 3x3 identity matrix\n    D1,D2,D3 = three 3x3 matrices (the 27 'new' parameters)\n               * when all 27 parameters == 0, warp is purely affine\n     {P|Q|R} = 3x3 matrix formed by adjoining the 3-vectors P,Q,R\n    inv[...] = inverse 3x3 matrix of stuff inside '[...]'\n* The inverse of a bilinear transformation is another bilinear\n   transformation.  Someday, I may write a program that will let\n   you compute that inverse transformation, so you can use it for\n   some cunning and devious purpose.\n* If you expand the inv[...] part of the above formula in a 1st\n   order Taylor series, you'll see that a bilinear warp is basically\n   a quadratic warp, with the additional feature that its inverse\n   is directly computable (unlike a pure quadratic warp).\n* 'bilinearD' means the matrices D1, D2, and D3 with be constrained\n  to be diagonal (a total of 9 nonzero values), rather than full\n  (a total of 27 nonzero values).  This option is much faster.\n* Is '-nwarp bilinear' useful?  Try it and tell me!\n* Unlike a bilinear warp, the polynomial warps cannot be exactly\n  inverted.  At some point, I'll write a program to compute an\n  approximate inverse, if there is enough clamor for such a toy.\n\n===========================================================================\n\n =========================================================================\n* This binary version of 3dAllineate is compiled using OpenMP, a semi-\n   automatic parallelizer software toolkit, which splits the work across\n   multiple CPUs/cores on the same shared memory computer.\n* OpenMP is NOT like MPI -- it does not work with CPUs connected only\n   by a network (e.g., OpenMP doesn't work with 'cluster' setups).\n* For implementation and compilation details, please see\n   https://afni.nimh.nih.gov/pub/dist/doc/misc/OpenMP.html\n* The number of CPU threads used will default to the maximum number on\n   your system. You can control this value by setting environment variable\n   OMP_NUM_THREADS to some smaller value (including 1).\n* Un-setting OMP_NUM_THREADS resets OpenMP back to its default state of\n   using all CPUs available.\n   ++ However, on some systems, it seems to be necessary to set variable\n      OMP_NUM_THREADS explicitly, or you only get one CPU.\n   ++ On other systems with many CPUS, you probably want to limit the CPU\n      count, since using more than (say) 16 threads is probably useless.\n* You must set OMP_NUM_THREADS in the shell BEFORE running the program,\n   since OpenMP queries this variable BEFORE the program actually starts.\n   ++ You can't usefully set this variable in your ~/.afnirc file or on the\n      command line with the '-D' option.\n* How many threads are useful? That varies with the program, and how well\n   it was coded. You'll have to experiment on your own systems!\n* The number of CPUs on this particular computer system is ...... 2.\n* The maximum number of CPUs that will be used is now set to .... 2.\n* OpenMP may or may not speed up the program significantly.  Limited\n   tests show that it provides some benefit, particularly when using\n   the more complicated interpolation methods (e.g., '-cubic' and/or\n   '-final wsinc5'), for up to 3-4 CPU threads.\n* But the speedup is definitely not linear in the number of threads, alas.\n   Probably because my parallelization efforts were pretty limited.\n =========================================================================\n\n++ Compile date = May 25 2018 {AFNI_18.1.18:macosx_10.7_Intel_64}", "help_range": [53694, 66902]}]}