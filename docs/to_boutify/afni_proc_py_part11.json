{"helptext": ["n the most basic form, with no other options.", "", "                afni_proc.py -ask_me", "", "        a2. Supply input datasets.", "", "                afni_proc.py -ask_me -dsets ED/ED_r*.HEAD", "", "        a3. Same as a2, but supply the datasets in expanded form.", "            No suffix (.HEAD) is needed when wildcards are not used.", "", "                afni_proc.py -ask_me                          \\", "                     -dsets ED/ED_r01+orig ED/ED_r02+orig     \\", "                            ED/ED_r03+orig ED/ED_r04+orig     \\", "                            ED/ED_r05+orig ED/ED_r06+orig     \\", "                            ED/ED_r07+orig ED/ED_r08+orig     \\", "                            ED/ED_r09+orig ED/ED_r10+orig", "", "        a4. Supply datasets, stim_times files and labels.", "", "                afni_proc.py -ask_me                                    \\", "                        -dsets ED/ED_r*.HEAD                            \\", "                        -regress_stim_times misc_files/stim_times.*.1D  \\", "                        -regress_stim_labels ToolMovie HumanMovie       \\", "                                             ToolPoint HumanPoint", "", "", "", "    ==================================================", "    Many NOTE sections: ~1~", "    ==================================================", "", "    --------------------------------------------------", "    GENERAL ANALYSIS NOTE: ~2~", "", "    How might one run a full analysis?  Here are some details to consider.", "", "    0. Expect to re-run the full analysis.  This might be to fix a mistake, to", "       change applied options or to run with current software, to name a few", "       possibilities.  So...", "", "         - keep permanently stored input data separate from computed results", "           (one should be able to easily delete the results to start over)", "         - keep scripts in yet another location", "         - use file naming that is consistent across subjects and groups,", "           making it easy to script with", "", "    1. Script everything.  One should be able to carry out the full analysis", "       just by running the main scripts.", "", "       Learning is best done by typing commands and looking at data, including", "       the input to and output from said commands.  But running an analysis for", "       publication should not rely on typing complicated commands or pressing", "       buttons in a GUI (graphical user interface).", "", "         - it is easy to apply to new subjects", "         - the steps can be clear and unambiguous (no magic or black boxes)", "         - some scripts can be included with publication", "           (e.g. an afni_proc.py command, with the AFNI version)", "", "         - using a GUI relies on consistent button pressing, making it much", "           more difficult to *correctly* repeat, or even understand", "", "    2. Analyze and perform quality control on new subjects promptly.", "", "         - any problems with the acquisition would (hopefully) be caught early", "         - can compare basic quality control measures quickly", "", "    3. LOOK AT YOUR DATA.  Quality control is best done by researchers.", "       Software should not be simply trusted.", "", "         - afni_proc.py processing scripts write guiding @ss_review_driver", "           scripts for *minimal* per-subject quality control (i.e. at a", "           minimum, run that for every subject)", "         - initial subjects should be scrutinized (beyond @ss_review_driver)", "", "         - concatenate anat_final datasets to look for consistency", "         - concatenate final_epi datasets to look for consistency", "         - run gen_ss_review_table.py on the out.ss_review*.txt files", "           (making a spreadsheet to quickly scan for outlier subjects)", "", "         - many issues can be detected by software, buy those usually just come", "           as warnings to the researcher", "         - similarly, some issues will NOT be detected by the software", "         - for QC, software can assist the researcher, not replace them", "", "         NOTE: Data from external sites should be heavily scrutinized,", "               including any from well known public repositories.", "", "    4. Consider regular software updates, even as new subjects are acquired.", "       This ends up requiring a full re-analysis at the end.", "", "       If it will take a while (one year or more?) to collect data, update the", "       software regularly (weekly?  monthly?).  Otherwise, the analysis ends up", "       being done with old software.", "", "          - analysis is run with current, rather than old software", "          - will help detect changes in the software (good ones or bad ones)", "          - at a minimum, more quality control tools tend to show up", "          - keep a copy of the prior software version, in case comparisons are", "            desired (@update.afni.binaries does keep one prior version)", "          - the full analysis should be done with one software version, so once", "            all datasets are collected, back up the current analysis and re-run", "            the entire thing with the current software", "          - keep a snapshot of the software package used for the analysis", "          - report the software version in any publication", "", "    5. Here is a sample (tcsh) script that might run a basic analysis on", "       one or more subjects:", "", "       ======================================================================", "       sample analysis script ~3~", "       ======================================================================", "", "       #!/bin/tcsh", "", "       # --------------------------------------------------", "       # note fixed top-level directories", "       set data_root = /main/location/of/all/data", "", "       set input_root = $data_root/scanner_data", "       set output_root = $data_root/subject_analysis", "", "       # --------------------------------------------------", "       # get a list of subjects, or just use one (consider $argv)", "       cd $input root", "       set subjects = ( subj* )", "       cd -", "", "       # or perhaps just process one subject?", "       set subjects = ( subj_017 )", "", "", "       # --------------------------------------------------", "       # process all subjects", "       foreach subj_id ( $subjects )", "", "          # --------------------------------------------------", "          # note input and output directories", "          set subj_indir = $input_root/$subj_id", "          set subj_outdir = $output_root/$subj_id", "", "          # --------------------------------------------------", "          # if output dir exists, this subject has already been processed", "          if ( -d $subj_outdir ) then", "             echo \"** results dir already exists, skipping subject $subj_id\"", "             continue", "          endif", "", "          # --------------------------------------------------", "          # otherwise create the output directory, write an afni_proc.py", "          # command to it, and fire it up", "", "          mkdir -p $subj_outdir", "          cd $subj_outdir", "", "          # create a run.afni_proc script in this directory", "          cat > run.afni_proc << EOF", "", "          # notes:", "          #   - consider different named inputs (rather than OutBrick)", "          #   - verify how many time points to remove at start (using 5)", "          #   - note which template space is preferable (using MNI)", "          #   - consider non-linear alignment via -tlrc_NL_warp", "          #   - choose blur size (using FWHM = 4 mm)", "          #   - choose basis function (using BLOCK(2,1), for example)", "          #   - assuming 4 CPUs for linear regression", "          #   - afni_proc.py will actually run the proc script (-execute)", "", "", "          afni_proc.py -subj_id $subj_id                          \\", "              -blocks tshift align tlrc volreg blur mask regress  \\", "              -copy_anat $subj_indir/anat+orig                    \\", "              -dsets                                              \\", "                  $subj_indir/epi_r1+orig                         \\", "                  $subj_indir/epi_r2+orig                         \\", "                  $subj_indir/epi_r3+orig                         \\", "              -tcat_remove_first_trs 5                            \\", "              -align_opts_aea -cost lpc+ZZ                        \\", "              -tlrc_base MNI152_T1_2009c+tlrc                     \\", "              -tlrc_NL_warp                                       \\", "              -volreg_align_to MIN_OUTLIER                        \\", "              -volreg_align_e2a                                   \\", "              -volreg_tlrc_warp                                   \\", "              -blur_size 4.0                                      \\", "              -regress_motion_per_run                             \\", "              -regress_censor_motion 0.3                          \\", "              -regress_reml_exec -regress_3dD_stop                \\", "              -regress_stim_times                                 \\", "                  $stim_dir/houses.txt                            \\", "                  $stim_dir/faces.txt                             \\", "                  $stim_dir/doughnuts.txt                         \\", "                  $stim_dir/pizza.txt                             \\", "              -regress_stim_labels                                \\", "                  house face nuts za                              \\", "              -regress_basis 'BLOCK(2,1)'                         \\", "              -regress_opts_3dD                                   \\", "                  -jobs 4                                         \\", "                  -gltsym 'SYM: house -face' -glt_label 1 H-F     \\", "                  -gltsym 'SYM: nuts -za'    -glt_label 2 N-Z     \\", "              -regress_est_blur_errts                             \\", "              -execute", "", "          EOF", "          # EOF denotes the end of the run.afni_proc command", "", "          # now run the analysis (generate proc and execute)", "          tcsh run.afni_proc", "", "       # end loop over subjects", "       end", "", "       ======================================================================", "", "    --------------------------------------------------", "    QUALITY CONTROL NOTE: ~2~", "", "    Look at the data.", "", "    Nothing replaces a living human performing quality control checks by", "    looking at the data.  And the more a person looks at the data, the better", "    they get at spotting anomalies.", "", "    There are 2 types of QC support generated by afni_proc.py, scripts to help", "    someone review the data, and individual text or image files.", "", "        ----------------------------------------------------------------------", "        scripts (the user can run from the results directory):", "", "           @epi_review.FT               - view original (post-SS) EPI data", "           @ss_review_basic             "], "params": [{"param_range": [1000, 1020], "help_range": [976, 7613]}, {"param_range": [7628, 7634], "help_range": [7614, 7953]}, {"param_range": [7968, 7983], "help_range": [7954, 8633]}, {"param_range": [8648, 8667], "help_range": [8634, 8973]}, {"param_range": [8988, 9008], "help_range": [8974, 9177]}, {"param_range": [9192, 9209], "help_range": [9178, 9517]}], "previous": "afni_proc.py_part10.json", "next": "afni_proc.py_part12.json"}